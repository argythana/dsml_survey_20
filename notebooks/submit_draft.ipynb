{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xarray as xa\n",
    "import importlib\n",
    "from typing import List\n",
    "import holoviews as hv\n",
    "from IPython.core.display import display, HTML\n",
    "import natsort\n",
    "np.set_printoptions(linewidth=200)\n",
    "pd.options.display.max_columns = None\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from IPython.lib import deepreload\n",
    "\n",
    "import kagglelib as kglib\n",
    "\n",
    "class disabled_print:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "# https://stackoverflow.com/questions/28101895/reloading-packages-and-their-submodules-recursively-in-python\n",
    "def reload_kglib() -> None:\n",
    "    with disabled_print():\n",
    "        deepreload.reload(kglib, exclude={key for (key, value) in sys.modules.items() if \"kagglelib\" not in key})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: where the truth lies OR # Guitar or Drums?\n",
    "# State of Machine Learning and Data Science 2020, Revisited\n",
    "\n",
    "### Table of Contents\n",
    "0. Introduction\n",
    "1. Methodology and key differences\n",
    "2. EDA for filtering \"pollution\", spam and user error\n",
    "3. Data Scientists Profile\n",
    "4. Data Scientists Programming and ML Experience\n",
    "5. Salary DS and other roles(company size?)\n",
    "6. (Technology ?)\n",
    "7. Conclusion\n",
    "8. Appendix A: Data cleaning methodology, assumption, datasets, functions\n",
    "9. Appendix B: Exploring Q & A; suggestions for future Surveys.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Kaggle DS & ML Survey is an open online survey receiving thousands of responses from all over the world, offering unique insights.  \n",
    "Being a global online survey it is affected by a factor that Machine Learning is famous for mitigating: spam, and user error.  \n",
    "According to the Survey Methodology, \"spam\" has been excluded from the data.\n",
    "\n",
    "Using Exploratory Data Analysis (EDA) we find that without dropping possible outliers, a large part of the data should be excluded for being spam or for having abnormal, irregular values.  \n",
    "Using relatively tolerant criteria, we identify 3859 such observations (19% of the dataset) and after filtering them we get significantly different results than the \"Executive Summary\" and all other EDAs, that do not filter the data.\n",
    "\n",
    "Focusing on the issue of compensation, we account for the wide global diversity by grouping countries according to the World Bank \"Income Groups\" definition.  \n",
    "We find that the median salary is higher to a significant degree and the distribution is shifted towards higher salary thresholds.  \n",
    "For example, the median salary for India-based Data Scientists is double than what the unfiltered data suggest.\n",
    "\n",
    "We also reconstruct the various classes (bins) in which the data are provided, accounting for their width and their underlying properties.  \n",
    "We find that the age distribution density is much higher in the younger groups than the unadjusted bins show.\n",
    "\n",
    "The weight of USA observations is disproportional, so we exclude the USA from global or group aggregates, offering a different view on the pronounced symmetric difference.  \n",
    "US-based data scientists are more experienced and for the same levels of experience, they earn almost double the amount than their colleages residing in other high income countries.\n",
    "\n",
    "Combining these four features, our results diverge to a large extend from the \"Executive Summary\", and all other analyses that do not filter the data, especially with respect to our focus, the issue of compensation levels.\n",
    "\n",
    "Our findings add to the understanding of the data, providing information to students, professionals and interested companies in order to optimize their strategical behavior with respect to compensation, relocation and remote work.  \n",
    "At the same time, we provide a fundamentally different methodological framework on a number of issues.\n",
    "\n",
    "To explore various scenarios we have created a dedicated python \"library\" with functions, which may be parameterized and reused for various filtering thresholds and different data subsets.  \n",
    "The whole notebook (other than the text) can be reparametarized by setting one different argument (or more).\n",
    "\n",
    "In Appendix A we discuss in detail out methodology and the datasets we use.\n",
    "\n",
    "In Appendix B, we provide another distinct result of our exploratory analysis;\n",
    "a concise list of suggestions for future surveys.\n",
    "\n",
    "# 1. Methodology and key differences\n",
    "\n",
    "\n",
    "### a) Exploring the questions, the answer choices and the dataset for spam, errors and unacceptable values\n",
    "\n",
    "The first part of our EDA is dedicated to exploring abnormal/invalid observation values that have not be flagged as spam by the survey system, but should be definetely excluded from the data.  \n",
    "We use the official \"Executive Summary\" as a benchmark but our results also differ from all other EDAs that explore the data without filtering them first.\n",
    "\n",
    "The number of unacceptable values and the criteria that we had to set, highlight the principal importance and challenges of exploring the data for errors and cleaning them before embarking on any other kind of analysis.\n",
    "\n",
    "Besides unacceptable values, we find mispecifications in \"Questions\" and \"Answer Choices\" that may lead to unfounded conclusions if they go unnoticed.\n",
    "\n",
    "\n",
    "\n",
    "### b) Adjusting for cross-country economic differences\n",
    "\n",
    "The importance of meaningful information on compensation levels cannot be understated in a field that transcends national borders like few others.  \n",
    "Nevertheless, the differences among different economies are so profound that a global view on Data Scientist salary distribution shows an \"everything goes\" result.\n",
    "\n",
    "Besides a robust single country median calculation, we group the countries according to the World Bank Income Groups and explore the salary distribution per group of countries. This allows us in turn to explore in a meaningful way the effect that experience levels have on salary in each countries income group. We select \"experience\" as the variable that may explain differences in salary levels and find that it leads to significant within-group differences for all country groups and countries.\n",
    "\n",
    "\n",
    "\n",
    "### c) Reconstruction of data aggregated bins (classes)\n",
    "\n",
    "Using different classes width for aggregating observations is convenient and informative.  \n",
    "But convenience comes with a trade-off; a large amount of information is lost.  \n",
    "If bins are created unequal, this may lead to \"over\" or \"under\" representation of data, depending on the relative width difference and the number of observations.  \n",
    "It may  also distort provided information and create optical \"illusions\" in visual representations.\n",
    "\n",
    "We reconstruct bins using various ways where we find this phenomenon to exist in the data and we gain new and useful information.\n",
    "\n",
    "\n",
    "\n",
    "### d) Examine USA versus Rest of World (RoW) symmetric differences\n",
    "\n",
    "When comparing the aggregate metrics of a subset of the data with the overall aggregates we examine whether the subset in question influences heavily the aggregated values.  \n",
    "It is informative to compare a subset with low number of observations against the total, to asses how it measures against it.\n",
    "\n",
    "But, for a subset with substantial weight, information is distorted since the subset defines the total to a significant degree.  \n",
    "In that case, comparing the symmetric difference can add valuable information for both sides. Therefore, when comparing the USA, we exclude it from the total.\n",
    "\n",
    "## 2. EDA for filtering \"pollution\", spam and user error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load unfiltered Kaggle data with optimised labels andcolumns for EDA\n",
    "udf = kglib.load_udf()\n",
    "\n",
    "seconds = [30, 45, 60, 90]\n",
    "super_fast_participants = pd.DataFrame(\n",
    "    {\n",
    "        \"seconds\": seconds,\n",
    "        \"fast_participants\": [len(udf[udf.duration <= sec]) for sec in seconds],\n",
    "    }, \n",
    ")\n",
    "# super_fast_participants\n",
    "\n",
    "dataset = udf\n",
    "fastest = dataset.duration.nsmallest(1).values[0]\n",
    "fastest_time_n = dataset.duration.nsmallest(300).tail(1).values[0]\n",
    "slowest = dataset.duration.nlargest(1).values[0]\n",
    "slowest_time_n = dataset.duration.nlargest(300).tail(1).values[0]\n",
    "#fastest, fastest_time_n, (slowest/3600),  (slowest_time_n/3600)\n",
    "\n",
    "# (udf.duration <= 3000).sum()\n",
    "# udf[udf.duration < 3000].duration.hist(bins=200, grid=False)\n",
    "# udf.duration.median()\n",
    "# udf.duration.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference of our analysis is that the first part of our EDA is dedicated to exploring abnormal/invalid observation values that have not be flagged as spam by the survey system, but should definetely be excluded from the data.  \n",
    "The number of unacceptable values and the criteria that we had to set, highlight the complexity, the challenges but mostly the principal importance of exploring the data and cleaning them before embarking on any other kind of analysis.  \n",
    "Examining the very first variable we find that there are 347 participants that completed the survey in less than 60 seconds.  That is arguably not enough time to read the questions.\n",
    "\n",
    "| threshold (sec) | No. participants |\n",
    "|-----------------|------------------|\n",
    "| 30              |  29              |\n",
    "| 45              | 166              |\n",
    "| 60              | 347              |\n",
    "| 90              | 630              |\n",
    "\n",
    "\n",
    "\n",
    "The fastest \"participation\" time is 20 seconds, the 300th faster is 56 seconds, meaning that 300 participants \"completed\" the Survey in 56 seconds or less.  \n",
    "The slowest survey completion time is 318 hours (almost two weeks) and the 300th slowest time is 48 hours.  \n",
    "300 participants \"completed\" the Survey in more than two days.  \n",
    "Nevertheless, instead of choosing a reasonable minimum and maximum duration thresholds, we use other qualitative criteria, and we manage to filter out a large part of these observations.\n",
    "\n",
    "### The criteria we use to clean the data are:\n",
    "\n",
    "### Criterion 1: Participants who did not actually participate in the survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# udf.columns[:7].values\n",
    "\n",
    "orig_df = kglib.load_orig_kaggle_df()\n",
    "temp_df = orig_df.iloc[:, 7:]\n",
    "\n",
    "only_answer_demographic = (\n",
    "    (temp_df == 'None')\n",
    "    | temp_df.isnull()\n",
    ").all(axis=1)\n",
    "\n",
    "#len(orig_df[only_answer_demographic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes observations with values only for the first set of general demographic questions and no other relevant information that adds any value concerning DS & ML.  \n",
    "We drop these irrelevant observation values as they do not provide further information relative to the scope of the survey.  \n",
    "We could also add a threshold for minimum 3 Not-Nan values, but being lenient, we merely drop those who only answered demographic questions, plus the next one and then quit the survey.\n",
    "\n",
    "Using our first criterion, we identify and drop 1082 observations.  \n",
    "Note that, not using criterion 1 does not affect our main results with respect to salary levels at all.\n",
    "\n",
    "### Criterion 2: Participants that are too young for their experience (in Programming or Machine Learning).\n",
    "\n",
    "#### Below we show some examples of that criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "is_too_young_for_experience = (\n",
    "    (udf.age <= \"24\")\n",
    "    & (udf.code_exp == \"20+\")\n",
    ")\n",
    "udf[is_too_young_for_experience].iloc[:,1:7].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it is impossible to be 24 years old or less and have 20+ years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion 3: Participants whose salary, experience, age and country of residence are mutually exclusive.\n",
    "\n",
    "As an example, it is impossible to be less than 21 years old, working as an employee and earn above 500,000 usd as a yearly salary, in any country in the world.  \n",
    "A closer examination of these observations shows that spam often is extensive.  \n",
    "\n",
    "#### Below we show some examples of that criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "too_young_high_salary = udf[\n",
    "    (udf.age <= \"21\")\n",
    "    & (udf.salary.isin([\"300000-499999\", \"500000-999999\"]))\n",
    "].dropna(axis=1).head()\n",
    "too_young_high_salary.iloc[:,1:11].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likewise, as shown in the data below, it is impossible to reside in the US, have more than 10 years experience and earn less than 1000 usd per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "high_exp_bins = [\"10-20\", \"20+\"]\n",
    "is_high_exp = (\n",
    "    udf.code_exp.isin(high_exp_bins)\n",
    "    | udf.ml_exp.isin(high_exp_bins)\n",
    ")\n",
    "# udf[is_high_exp]\n",
    "\n",
    "extemely_low_salary_high_exp = (\n",
    "    (is_high_exp) &\n",
    "    (udf.salary_threshold <= 1000)\n",
    ")\n",
    "# udf[is_low_salary_high_exp].Mdropna(axis=1)\n",
    "\n",
    "USA_is_low_salary_high_exp = (\n",
    "    udf[\n",
    "        (extemely_low_salary_high_exp)\n",
    "        & (udf.country == \"USA\")\n",
    "    ].dropna(axis=1).iloc[:, 3:20]\n",
    ").dropna(axis=0).head(3)\n",
    "USA_is_low_salary_high_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple filtering we could drop hundreds of observations on the spot.  \n",
    "To move beyond simple filtering, and take into account the wide global differnces, we construct various combinations of age, salary, country and experience, taking the following steps:\n",
    "\n",
    "* a) we obtain the annual average salary for each country (\"country_avg_salary\") using official sources where available.\n",
    "\n",
    "Then, to avoid excluding outliers we set:\n",
    "\n",
    "* b) the corresponding \"salary_threshold\" for each observation, as the upper bound of Kaggle data salary bins.\n",
    "\n",
    "* c) the country adjusted \"low_salary_high_exp\" threshold, two thresholds below the country average, applicable only for experience values greater or equal to 10 years. \n",
    "   \n",
    "* d) the country adjusted \"too_low_salary\" threshold. We multiply the country average by 0.33 and set the rejection rate to be two thresholds below.  \n",
    "    \n",
    "* e) the \"high_salary_low_exp\" threshold, which is above 300000 for all countries.\n",
    "\n",
    "For a detailed account of the various third party data sets and the methodology applied, please read the Appendix A or review the submitted code.  \n",
    "As a side note, our thresholds leave more than enough room for part-time employeed participants.  \n",
    "Since, the question explicitly asks for yearly salary, observations that contain the value of very limited part-time salary as the yearly value, inhibit any meanigful exploration of proper yearly salary data.  \n",
    "To the extent that such values are present in the dataset, they are the result of user or design error and should be excluded from the data, evenmore so when examing salary distributions and factors that might explain it.  \n",
    "In order to get a comprehensive view of the data, and benchmark the filtering method, we use these observations to compare the difference between the two data sets (filtered, unfiltered).\n",
    "\n",
    "### In the table below we show a sample of the thresholds we calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# kglib.get_threshold(65800, 0)\n",
    "#kglib.get_threshold(65800 * 1/3, 2)\n",
    "\n",
    "\n",
    "tresholds_df = kglib.load_thresholds_df()\n",
    "usa_thresholds = tresholds_df[tresholds_df.country == \"USA\"]\n",
    "#usa_thresholds\n",
    "tresholds_df.sample(5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For country category \"Other\", we set the minimum thresholds, since the country names that would allow us to obtain each county's average salary are missing.  \n",
    "Our classification rules are intentionally too lenient and could most probably be much stricter.  \n",
    "This can be done on the spot, by setting different parameters in the relevant dedicated filtering function.  \n",
    "We explored various scenaria and we demonstrate a scenario on the very lenient spectrum, with respect to filtering.  \n",
    "Nevertheless, even with minimum strictness we get significantly different results than any other EDA that uses the data as is.  \n",
    "\n",
    "\n",
    "The significance of these values depends on the overall size of the subset they belong to and the respective calculated metric.  \n",
    "For example, a salary of 500,000 outweighs 100 observations of a salary of 5000.  \n",
    "Similarly, dropping 10 observations from a range of 15 observations, means that the size of the category is one third of its initial unfiltered size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fdf = kglib.filter_df(udf, print_filters=False)\n",
    "#filtered_obs_size = len(udf) - len(fdf)\n",
    "#filtered_obs_size,  filtered_obs_size/len(udf), len(fdf)\n",
    "\n",
    "dataset = fdf\n",
    "fastest = dataset.duration.nsmallest(1).values[0]\n",
    "fastest_time_n = dataset.duration.nsmallest(300).tail(1).values[0]\n",
    "slowest = dataset.duration.nlargest(1).values[0]\n",
    "slowest_time_n = dataset.duration.nlargest(300).tail(1).values[0]\n",
    "# fastest, fastest_time_n, (slowest/3600),  (slowest_time_n/3600)\n",
    "# pros_fdf.loc[pros_fdf.index == 16761].dropna(axis=1)\n",
    "\n",
    "pros_fdf = fdf[\n",
    "    (fdf.role != \"Student\")\n",
    "    & ((fdf.role != \"Currently not employed\"))\n",
    "]\n",
    "\n",
    "dataset = pros_fdf\n",
    "fastest_completion = dataset.duration.nsmallest(1).values[0]\n",
    "time_to_complete_faster_thousand = dataset.duration.nsmallest(1000).tail(1).values[0]\n",
    "slowest_completion = dataset.duration.nlargest(1).values[0]\n",
    "time_to_complete_slower_thousand = dataset.duration.nlargest(1000).tail(1).values[0]\n",
    "# fastest_completion, time_to_complete_faster_thousand, (slowest_completion/3600), (time_to_complete_slower_thousand/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the values that we filter are part of the same observation row, a fact that adds support to the decision to exclude them from the data.  \n",
    "As a result, the number of observations that we drop using each filter depends on the order the filters are applied.\n",
    "\n",
    "#### In total, we drop 3859 observations, 19% of the unfiltered data set.\n",
    "\n",
    "#### The filtered data set contains 16177 rows.\n",
    "\n",
    "\n",
    "After filtering the data, duration times are not so extreme, but we still have values that could be dropped if we set lenient min and max time criteria.  \n",
    "The two weeks completion time observation is still in the data, the fastest participant took 48 seconds, 300 participants made it in 123 seconds or less.\n",
    "\n",
    "If we exclude \"Students\" and \"Curenty not Employeed\" duration times are more reasonable.  \n",
    "The fastest participant completed the Survey in 49 seconds, the 300 faster participants took less than 5 minutes and the 300 slower made it after more than 19,5 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Scientists Demographic Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "uds = kglib.load_role_df(udf, role=\"Data Scientist\")\n",
    "fds = kglib.load_role_df(fdf, role=\"Data Scientist\")\n",
    "udf, uds, fdf, fds = map(kglib.keep_demo_cols, (udf, uds, fdf, fds))\n",
    "dataset1, dataset2 = uds, fds\n",
    "# len(fdf), len(fds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After filtering the data, the dataset contains 16177 observations.\n",
    "#### of which, 2090 are Data Scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Gender\n",
    "\n",
    "#### Men percentage is 1% higher compared to the unfiltered data,\n",
    "marginally better in terms of gender gap than the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "column = \"gender\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"No. participants\",\n",
    "    as_percentage=True,\n",
    "    order=[\"Man\", \"Woman\", \"Nonbinary\", \"No answer\", \"Self-describe\"],\n",
    ")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df=df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    legend_location=\"center right\",\n",
    "    title=\"Gender identity of data scientists, %\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Age\n",
    "\n",
    "#### Data scientists in the workplace, in the age of 18-21, are not that many after all.\n",
    "Still more than 60% of Data Scientists are below the age of 35 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "column = \"age\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"No participants\",\n",
    "    as_percentage=True\n",
    ")\n",
    "# df\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=12,\n",
    "    height=12,\n",
    "    orientation=\"v\",\n",
    "    title=\"Age distribution of Data Scientists, %\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "There is something more interesting concerning the whole survey distribution if we examine the bin widths.  \n",
    "The age bin of 18-21 spans over 4 years, the bin 22-24 over 3 years, and the 25-29 bin spans over 5 years.  \n",
    "This ad hoc bin creation is informative if you focus on any of these bins separately, but distorts the comparison between the three.  \n",
    "The relative size of the 18-21 bin is by definition augmented, while the 22-24 bin is by definition much smaller than the 25-29 bin.\n",
    "\n",
    "Using, for simplicity, the average number of participants per year in each bin and assuming equal distribution among each bin years, we get a different story.  \n",
    "The information the \"adjusted bins\" graph conveys is that it can be read as a potential growth and popularity trend of Kaggle.\n",
    "\n",
    "Adjusting the age bins, we find that in the 20-24 cohort there are 30% more participants than in the 25-29 cohort, and this is a noteable future growth trend prediction.  \n",
    "Plus, there are a lot more participants in the 18-19 age cohort than meets the eye.   \n",
    "Note that the 18-19 bin spans over two years.   \n",
    "Again, taking the average per year, the 18-19 bin density is definetely greater than the 25-29 bin, showing that Kaggle is way more popular to \"newcomers\".  \n",
    "The results shown below hold whether we use filtered or unfilterd data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = fdf\n",
    "kglib.sns_plot_age_distribution(\n",
    "    dataset,\n",
    "    width=14,\n",
    "    height=10,\n",
    "    title=\"Age distribution of Survey participants, Default vs Adjusted bins, Filtered data\"\n",
    ")\n",
    "\n",
    "#for unfiltered data we set:\n",
    "# dataset = udf\n",
    "# kglib.sns_plot_age_distribution(dataset, width=14, height=12,title=\"Survey Age distribution - Default VS Proposed bins, Uniltered data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. country\n",
    "\n",
    "####  The difference between USA and India is narrow, the UK is third in terms of participants and the Netherlands are inclused in the top 15 countries. \n",
    "\n",
    "Ps. There are two different \"answer choices\" for Korea, 76 obs for \"Republic of Korea\" and 190 for \"South Korea\".  \n",
    "Since both are associated with South Korea, we aggregate them as South Korea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Korea = orig_df[orig_df.Q3.str.contains(\"Korea\")].groupby('Q3').size()\n",
    "# Korea\n",
    "\n",
    "dataset1, dataset2 = uds, fds\n",
    "column = \"country\"\n",
    "df = kglib.get_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    as_percentage=True\n",
    ")\n",
    "\n",
    "df = df[df.Filtered > 1.5]\n",
    "df = df.sort_values(by=\"Filtered\")\n",
    "df = kglib.stack_value_count_comparison(df, \"participants (%)\")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=16,\n",
    "    height=6,\n",
    "    orientation=\"v\",\n",
    "    order_by_labels=False,\n",
    "    x_ticklabels_rotation=30,\n",
    "    title=\"Top 15 countries in the number of Data Scientists, %\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Education\n",
    "\n",
    "#### After filtering the data, PhD holders are reduced by one third.\n",
    "\n",
    "There is a more important discovery though.  \n",
    "The question is set in a way that does not allow for definite inferences about the current level of education of participants.  \n",
    "Even if \"Data Scientists\" are not \"Students\", we can not be 100% certain whether they replied about the level of education they have already attained or about the one they plan to attain.  \n",
    "#### Therefore, we exclude the dimension of education from our analysis as any EDA based on information provided by this question would be unfounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "column = \"education\"\n",
    "order = [\n",
    "    'No answer',\n",
    "    'High school'\n",
    "    'Studies without a degree',\n",
    "    'Professional',\n",
    "    'Doctoral',\n",
    "    'Bachelor’s',\n",
    "    'Master’s',\n",
    "]\n",
    "\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=order,\n",
    ")\n",
    "\n",
    "reload_kglib()\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"% of Education levels or Educational Goals of Data Scientists?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Scientists Programming and ML Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Programming Experience USA vs RoW\n",
    "\n",
    "#### The percentage of Data Scientitsts in the US with more than 20 years of Programming Experience is double than in the RoW.  \n",
    "Similarly, more than the two thirds of Data Scientists in the US, have more than 5 years of experience.  \n",
    "On the other hand, there is a shortage of Experienced Data Scientists in the RoW, since more than 50% have less than 5 years of Programming Experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fds_usa = fds[fds.country == \"USA\"]\n",
    "fds_non_usa = fds[fds.country != \"USA\"]\n",
    "dataset1 = fds_usa\n",
    "dataset2 = fds_non_usa\n",
    "\n",
    "column = \"code_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    ")\n",
    "\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Programming XP, USA vs RoW %, Filtered data\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Using the unfiltered data and excluding the US from the global aggregate, we see that:  \n",
    "#### the percentage of Data Scientitsts in the US with more than 20 years of Programming Experience is almost triple than in the RoW.  \n",
    "The shortage of experienced data scientist in the RoW is manifested vividly, as more than 60% of Data Scientists in the RoW have less than 5 years of Programming Experience.  \n",
    "\n",
    "\n",
    "Ps. There is an error in the \"Executive Summary\", p.9 plot concerning the USA \"20+ years\" bin.  \n",
    "The plot shows 7.6% of participants while the actual unfiltered data value is 17.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "uds_usa = uds[uds.country == \"USA\"]\n",
    "uds_non_usa = uds[uds.country != \"USA\"]\n",
    "dataset1 = uds_usa\n",
    "dataset2 = uds_non_usa\n",
    "\n",
    "column = \"code_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    ")\n",
    "\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Programming XP, USA vs RoW %, Unfiltered data\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Machine learning experience USA vs RoW\n",
    "\n",
    "After filtering the data and excuding the USA from the global aggragate, we find that:  \n",
    "#### The relative differences in Data Scientists with experience of more than 10 years is higher compared to the global aggreagares that include the US.\n",
    "\n",
    "More than one in three respondents (34.5%) from the US have experience of 10 years or more, versus one in five (20.3%) in the RoW.  \n",
    "\n",
    "Ps. There are two errors in the \"Executive Summary\", p.10 plot.  \n",
    "The bin for \"10-15 years\" should be \"10-20 years\".\n",
    "The relative scale for the global bars at the bins of \"10-20 years\" and \"20 or more\" has been leveled to almost zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fds_usa = fds[fds.country == \"USA\"]\n",
    "fds_non_usa = fds[fds.country != \"USA\"]\n",
    "dataset1 = fds_usa\n",
    "dataset2 = fds_non_usa\n",
    "\n",
    "column = \"ml_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    "    #order=order.reverse()\n",
    ")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Machine Learning XP, USA vs RoW %, Filtered data\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (20.3 - 16.3)/16.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "Using the unfiltered data but excluding the USA from the global aggregates, we see that: \n",
    "#### Less than one in 6 respondents have experience of more than 5 years in the RoW (16.3%).  \n",
    "#### There is a relative reduction of around one fourth in the unfilterd versus unfiltered data, for the respective categories.\n",
    "\n",
    "Similarly, only 4.5% of Data Scientists in the ROW appear in the unfiltered data to have more than 5 years experience in Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "uds_usa = uds[uds.country == \"USA\"]\n",
    "uds_non_usa = uds[uds.country != \"USA\"]\n",
    "dataset1 = uds_usa\n",
    "dataset2 = uds_non_usa\n",
    "\n",
    "column = \"ml_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    "    #order=order.reverse()\n",
    ")\n",
    "\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Machine Learning XP, USA vs RoW %, Unfiltered data\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Global Salary distribution\n",
    "The unfiltered data global salary distribution exhibits an \"anything goes\" pattern with three slightly higher density modes which are partly related to unequal bin widhts.  \n",
    "Note that the \"100000-124999\" bin width is 2.5 times wider than the \"90000-99999\" bin width.  \n",
    "Similarly, if we construct equal size bins by joining the \"7500-9999\" and the \"5000-7499\" bins, or all the bins from \"1000 to 4999\" we will get higher values than the \"10000-14999\" bin.  \n",
    "Even worse, what literaly sticks out is the \"0-999\" bin suggesting that almost one in five Data Scientists globaly earn less than on thousand $USD per year.  \n",
    "\n",
    "Thankfully, this is related to spam, user-error or probably part-time employement values.  \n",
    "The picture is not as bleak after filtering the data.  \n",
    "The filtered data global salary distribution values increase significantly, _from 25 to 30 percentage points_, the density of the three main modes and of their neighboring bins relative to their unfiltered values.  \n",
    "The three modes indicate hidden patterns in the data resulting in this distribution shape.  \n",
    "Therefore, we examine the salary distribution:\n",
    "\n",
    "    a) of the countries with the higher number of participants in the Survey (USA, India, UK, Brazil). \n",
    "    These countries happen to belong in different World Bank Income groups.\n",
    "    \n",
    "    b) groups of countries according to the World Bank \"Income Group\" categories.  \n",
    "    There were no countries in the \"Lower\" income group in the data.  \n",
    "    Some participants from countries belonging to this group, might be have been classified in the \"Other\" country category.  \n",
    "    Unfortunately, this information is not available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    uds,fds,\n",
    "    width=8, height=8,\n",
    "    title=\"% of Global Salary distribution of Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. US salary distribution\n",
    "\n",
    "Examining the filtered data, we get a clearer idea about what to expect working as a data scientist in the US.  \n",
    "We should expect, 97% of the times, a minimum staring salary above 70000.  \n",
    "We find that the US salary distribution is even more concentrated in the 90k to the 200k range.  \n",
    "Resembling one standard deviation analysis without calculating the mean, we find that more than 70% of salary observations are in the 90k to the 200k range.  \n",
    "The reason we choose the 90-99k bin instead of the 200-249k bin in order to calculate the cumulative peercentage of 70% of observations has to do with the relative bin widths as discussed previously.  \n",
    "\n",
    "The 100-125k bin retains its dominant position even if we combine the three bins below it that surpass the 100-125k bin width if we joined them.  \n",
    "We conclude that the mode of the distibution is the 100k-124999 bin, while the median would probably lie in the 125k-14999 bin.  \n",
    "The 150-199k bin density is double that the 200-249k bin which has equal witdh but its width is double than the previous two bins.  \n",
    "While it is a fact that most of the obseravations (23%) are in this range, the other fact is that axis does not have equal scale, creating a kind of visual \"illusion\".  \n",
    "This should be noted so that readers get a better understaning of the data.  \n",
    "Comparing the mode midpoint (112.5k) that we infer by examing the data and the bins, we find that it is higher by two thirds than the average US county salary as reported by the OECD and Eurostat (68700 + (68700*0.66)).  \n",
    "That is an information that all interested parties should keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_uds = uds[uds.country == \"USA\"]\n",
    "usa_fds = fds[fds.country == \"USA\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    usa_uds, usa_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 24), x2_limit=(0, 24),\n",
    "    title=\"% of Salary distribution of US-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c. India Salary distribution\n",
    "Examining the filtered data, we find that the Data Scientists salary distribution in India spreads out along a wider range of values.  \n",
    "As expected, after excluding data that suggested that one third of Data Scientists earn less than 25% of the country average, we can make more reliable inferences.  \n",
    "Note that there are no observations in the top salary bins despite setting lenient rejection thresholds.  \n",
    "This highlights the importance of filtering criteria that amy capture a small part of total data, since the might play an important role when examining subsets.\n",
    "\n",
    "Taking into account the unequal bin size we can see that 69.3% of the observations are in the 4k-4999 range.  \n",
    "This resonates with the fact the India is classified by the World Bank as a \"Lower Middle\" income country.  \n",
    "\n",
    "\n",
    "When we calculate the cumulative percentages of the distribution, we do not include the 40k-49999 bin. \n",
    "Instead, we include the 4k-4999 bin, despite the fact that both bins have similar density.  \n",
    "We do so because the 40-49999 bin is 10 times wider.  \n",
    "Simalarly, by examining bin widths, we find that the distribution density mode is at the 5k-9999 area (15.7%) including almost 10% more obsrevations than the 10k-14999 bin.  \n",
    "\n",
    "As in the USA, the mode bin midpoint (7500) is more than two thirds larger than the country average salary.  \n",
    "This evidence adds support to the general intuition that Data Scentists' salaries are considerably higher than the average country salaries.  \n",
    "It also indicates that our salary value rejection criteria were lenient indeed.  \n",
    "As a final note, by examining the filtered data graph, we find evidence that being intentionally lenient, we have allowed for a part of non-normal values to remain in the data in the 1000-1999 bin.  \n",
    "This also infulences the salary distribution and indicates that it might actually take higher percentage values.  \n",
    "Pointing out non-normal values should be noted since the identification of such values is a goal in itself in any EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "india_uds = uds[uds.country == \"India\"]\n",
    "india_fds = fds[fds.country == \"India\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    india_uds, india_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 32.5), x2_limit=(0, 32.5),\n",
    "    title=\"% of Salary distribution of India-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d. UK salary distribution\n",
    "\n",
    "The UK salary distribution is also spread out, but a closer look reveals that the 150k-199999 bin sticks out partly due to its relatively large width.  \n",
    "Still, 36% of values are above the 100k threshold and there is a two-modal pattern in the UK distribution.  \n",
    "Despite being classified by the World Bank as a \"High Income\" country, the UK is actually among the top countries in that category in terms of average salary.  \n",
    "Accounting for experience levels could perhaps explain salary differences more than age or education.\n",
    "\n",
    "Overall, the UK Data Scientists salary distribution takes values much lower than the in USA.  \n",
    "This fact, indicates that we should exclude the US from the group of \"High Income\" countries when examining the salary distribution of the group.  \n",
    "We took a similar approach when we examined the experience levels in the USA versus the RoW.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "uk_uds = uds[uds.country == \"UK\"]\n",
    "uk_fds = fds[fds.country == \"UK\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    uk_uds, uk_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 20), x2_limit=(0, 20),\n",
    "    title=\"% Salary distribution of UK-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d. Brazil salary distribution\n",
    "\n",
    "Brazil is classified by the World Bank as an \"Upper middle\" income country.\n",
    "\n",
    "The salary distributtion is significantly different than the other three countries we examined.\n",
    "\n",
    "We see that the majority of observations are in the 10k-59999 range and observation are more evenly distributed in the 10k to 40k range.\n",
    "\n",
    "The graph indicates that some possible spam and invalid values remain in the \"1000-1999\" bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "brazil_uds = uds[uds.country == \"Brazil\"]\n",
    "brazil_fds = fds[fds.country == \"Brazil\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    brazil_uds, brazil_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 20), x2_limit=(0, 20),\n",
    "    title=\"% Salary distribution of Brazil-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5e. Comparing Medians and section conclusions\n",
    "\n",
    "Comparing the median salaries of the filtered versus the unfiltered data we find a significant difference for most countries.  \n",
    "\n",
    "Taking the bin midpoints, we find that the median salary is higher by:\n",
    "\n",
    "    44% in Japan,\n",
    "    22% in France,\n",
    "    40% in Russia,\n",
    "    40% in Brazil,\n",
    "    100% in India,\n",
    "   than the unfiltered data suggest.  \n",
    "\n",
    "This graph confirms the signifiacnt difference between the US and other countries, in terms of Data Scientists salary.  \n",
    "There results are similar for the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kglib.load_salary_medians_df(\n",
    "    dataset1=uds,\n",
    "    dataset2=fds,\n",
    "    label1=\"Unfiltered\",\n",
    "    label2=\"Filtered\",\n",
    "    countries = [\"USA\", \"Germany\", \"Japan\", \"France\", \"Russia\", \"Brazil\", \"India\"],\n",
    ")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df=df, \n",
    "    width=8, \n",
    "    height=8, \n",
    "    orientation=\"h\", \n",
    "    order_by_labels=False,\n",
    "    annotation_mapping=kglib.REVERSE_SALARY_THRESHOLDS,\n",
    "    title=\"Median salary for Data Scientists\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5f. Salary distributions for country \"Income groups\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the global salary distribution is so diverse, we group the countries in the data according to the World Bank \"Income group\" criteria, in order to examine possible differences in the distribution.  \n",
    "Please read Appendix A for relevant info on World Bank \"Income Groups\".  \n",
    "As a reminder, the data bins' width in the x-axis is not equal, in fact bins size increases by orders of magnitude.  \n",
    "We ameliorate this by using logarithmic scale and by adjusting the graph smoothing factor.  \n",
    "\n",
    "According to the Wordl Bank definition the Income groups comprises countries with wide income differences.  \n",
    "For example, the Wordl Bank \"High Income\" group definition includes Romania, Portugal, Germany, Switzerland and the USA.\n",
    "\n",
    "Ploting the salary by country income groups we get an informative decomposition of the distribution, in contrast to the \"everything goes\" global view.  \n",
    "We find that the US is a class of its own in the data, taking quite higher values than the rest of the \"High Income\" countries.  \n",
    "This is another confirmation that the USA merits a distinct examination and at the same, when making inference about the RoW the USA should be excluded.  \n",
    "Therefore, the \"High Income\" group in this graph does not include the USA.  \n",
    "The graph confirms the USA distribution mode is the one that we infer by examing the bins' width.  \n",
    "\n",
    "The main area of the \"High Income\" countries distribution appears to be after the mode of the \"Upper Middle\" income group but before the USA distribution mode.\n",
    "\n",
    "The \"Upper Middle\" countries distribution mode is positioned at the salary level of the mimimun outliers of the \"High Income\" distribution.  \n",
    "The left tail is most likely unfiltered spam.\n",
    "\n",
    "Since India comprises one fifth of the Data Scientists, we examine it separately and exclude it from the \"Lower Middle\" countries group to assess whether that makes a difference.  \n",
    "We find that India is somewhere in the middle between \"Upper Middle\" and \"Lowew Middle\" countries.  \n",
    "\n",
    "The \"Lower Middle\" countries salary levels are ditributed mostly to lower salary bins. The left tail might be related to spam or to countries with very low salary levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kglib.sns_plot_salary_pde_comparison_per_income_group(\n",
    "    fds,\n",
    "    width=9,\n",
    "    height=9,\n",
    "    log_scale=True,\n",
    "    title=\"Salary density estimation, per Income group, (log scale)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5g. Salary distributions for Income groups and occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_kglib()\n",
    "usa_fdf = fdf[fdf.country == \"India\"]\n",
    "\n",
    "kglib.sns_plot_salary_pde_comparison_per_role(\n",
    "    usa_fdf,\n",
    "    width=14,\n",
    "    height=18,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP from here , IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kglib.load_median_salary_per_income_group_per_XP_level_df(uds, xp_type=\"code\", income_group=\"3\")\n",
    "kglib.load_median_salary_per_income_group_per_XP_level_df(uds, xp_type=\"ml\", income_group=\"3\")\n",
    "kglib.load_median_salary_per_income_group_per_XP_level_df(uds, xp_type=\"ml\")\n",
    "\n",
    "df = kglib.load_median_salary_comparison_df(uds, fds, xp_type=\"code\", income_group=\"3\")\n",
    "kglib.sns_plot_value_count_comparison(df, height=8, width=18, bar_width=0.35, title_wrap_length=80, title=\"Data Scientists: Median salary per Code XP level in High Income countries Filtered vs Unfiltered datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP from here , IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fds_usa = fds[fds.country == \"USA\"]\n",
    "fds_non_usa = fds[fds.country != \"USA\"]\n",
    "\n",
    "reload_kglib()\n",
    "df = kglib.load_median_salary_comparison_df(fds_non_usa, fds_usa, xp_type=\"ml\", income_group=\"3\", label1=\"Non USA DS\", label2=\"USA DS\")\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df, height=8, width=18, bar_width=0.35, title_wrap_length=70, \n",
    "    title=\"Median salary per Machile Learning XP level (filtered dataset)\"\n",
    ")\n",
    "\n",
    "df = kglib.load_median_salary_comparison_df(fds_non_usa, fds_usa, xp_type=\"code\", income_group=\"3\", label1=\"Non USA DS\", label2=\"USA DS\")\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df, height=8, width=18, bar_width=0.35, title_wrap_length=70, \n",
    "    title=\"Median salary per Coding XP level (filtered dataset)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################\n",
    "\n",
    "## Appendix A: Data cleaning methodology, assumption, datasets, functions\n",
    "\n",
    "\n",
    "### a) Average salary data and assumptions\n",
    "\n",
    "The first thing that sticks out besides Survey \"completion\" time is the large number of observations with salary value in the 0-999 bin.  \n",
    "This holds even for the USA.  \n",
    "But, to set a reliable cut-off threshold in order to evaluate salary values we chose the average country salary.\n",
    "Undoubtly, an average consists of a range of values that fluctuate for a number of reasons, related to age, experience, cross-sectoral and within-sectoral differences, geographical differences within the same country and generally for a whole range of factors that are beyond our scope to examine.  \n",
    "Additionally, the country average salary may me skewed towards higher values, even more so for states with small total population and developed high-salary sectors.  \n",
    "Yet, it is more than acceptable to assume that the average salary of Data Scientists, _and all other occupations in this Survey_, is above the country average.  \n",
    "This is generally acknowledged but more importantly it is evident in the data, whether we filter it or not, and can be easily proven by examing one by one each country average.  \n",
    "\n",
    "This brings us to our two main issues. Average salary data availability and deriving a cut-off threshold.  \n",
    "Since, in our knowledge no such aggregate global dataset exists, we had to combine all available datasets, and adjusting for possible trade offs.\n",
    "\n",
    "For countries in the European Union, we use data for average salary from Eurostat.  \n",
    "We convert in to US dollar using the average 2019 official exchange the we download from the ECB.\n",
    "\n",
    "For other countries, we use OECD data which are reported in Purchasing Power Parity Levels.  \n",
    "Besides the US which acts as a benchmark, this increases or decreases the average salary depending on the \n",
    "\n",
    "* we obtain the annual average salary for each country (\"country_avg_salary\"), using official sources where available.  \n",
    "Eurostat, OECD ......\n",
    "\n",
    "\n",
    "\n",
    "### b) Assumptions and steps to set rejection thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the = kglib.load_mean_salary_comparison_df().sample(10).dropna(axis=0,thresh =3 )\n",
    "the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive examples of filtering by salary,age, experince by country:\n",
    "\n",
    "\n",
    "Then, to avoid excluding outliers we set:\n",
    "\n",
    "* b) the corresponding \"salary_threshold\" for each observation, as the upper bound of Kaggle data salary bins.\n",
    "    \n",
    "    E.g. for a salary value in the \"4000-4999\" bin the salary threshold we set is \"5000\".\n",
    "\n",
    "* c) the country adjusted \"low_salary_high_exp\" threshold, two thresholds below the country average, applicable only for experience value >= 10 years.\n",
    "    \n",
    "    E.g. for USA, average country salary is 65835, the bin threshold is \"60000-70000\" and the bin upper bound is 70000.  \n",
    "    So for participants from USA, with 10 or more years of experience, the \"low_salary_high_experience\" treshold is 50000.  \n",
    "    Accordingly, the rejection criterion drops values which are below the upper bin bound of 50000 (in the \"40000-49999\" range).  \n",
    "    In simple terms, for high experience, we drop observations with salary which are two thresholds below the country average salary upper threshold.\n",
    "   \n",
    "* d) the country adjusted \"too_low_salary\" threshold. We multiply the country average by 0.33 and set the rejection rate to be two thresholds below.  \n",
    "    \n",
    "    E.g. for USA residents the average country salary is 65800 and 1/3 of the country average salary is 21945, which is in the \"20000-29999\" bin.  \n",
    "    Accordingly, the upper theshold is 30000 and the rejection criterion is applied for values two thresholds below, in the range of \"10000-14999\".  \n",
    "    In short, the \"too low salary\" rejection criterion drops values two thresholds below the threshold of the one third of the country average.\n",
    "\n",
    "* e) the \"high_salary_low_exp\" threshold, which is above 300000 for all countries.\n",
    "\n",
    "    We argue that no one with zero or minimum experience is paid for working as a Data Scientist such a salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Setting arguments in the filtering function.\n",
    "\n",
    "As mentioned above, the data are filtered using a dedicated function that can be reparameterized on the spot by setting:\n",
    "\n",
    "- a stricter (higher) or more lenient (lower) value for the \"low_salary_percentage\" argument,\n",
    "- a different value in the \"threshold_offset\" argument,\n",
    "- by adjusting what is considered to be high or low experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from ground zero, we examine the time that it took participant to complete the survey.\n",
    "\n",
    "Checking duration to complete and number of questions answered, we identified a large set of observations that completed the survey in less than 30 seconds and only answered the first set of basic demographic questions and no other information that would add value concerning the DS & ML Survey. We decided to drop these observations as they offer no information whatsoever about the issues in question, are spam or irrelevant.\n",
    "\n",
    "But, since meaningful differences in the results concerning DS & ML, originate in observations who spammed the answers instead of not answering them, we set logical thresholds for invalid values, on mutually exclusive value pairs and their combinations.\n",
    "\n",
    "\n",
    "There are hundreds of observations below 30 seconds. It is impossible to complete a survey of this length in 20 seconds. We could set an arbitrary time threshold here, but we since this points to some people not completing the Survey, we decided there is another way to check this.\n",
    "\n",
    "Conclusion 1: the spam system method includes participants which did not actually complete the Survey and answered whatever as fast as possible. We could use this criterion to drop many observations, but, we found an optimal one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Data Analysis driven suggestions for future Kaggle Surveys.\n",
    "\n",
    "    1. Set a mimimum time rejection limit and a maximum \"timeout\" limit.\n",
    "    2. Request Age input as integer values.\n",
    "    3. Education titles. Ask a separate question about current obtained title.\n",
    "    4. Education current studies. Ask separately about current studies to \"Students\" only.\n",
    "    5. Education, future plans. If that is important for the scope of the Survey ask it separately.\n",
    "    4. If employeed first ask about full or part-time employment.\n",
    "    5. Request Monthly instead of Yearly salary and provide hints to participants.\n",
    "    6. Request Salary input as integer values\n",
    "    7. This is not always True: Non-professionals were defined as students, unemployed, and respondents that have never spent any money in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
