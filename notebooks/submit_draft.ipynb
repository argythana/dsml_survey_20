{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "MYSITEPACKAGES = \"/kaggle/input/dsml-20/\"\n",
    "sys.path.insert(0, MYSITEPACKAGES)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.lib import deepreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from IPython.lib import deepreload\n",
    "\n",
    "import kagglelib as kglib\n",
    "\n",
    "# https://stackoverflow.com/questions/8391411/how-to-block-calls-to-print\n",
    "class disabled_print:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "# https://stackoverflow.com/questions/28101895/reloading-packages-and-their-submodules-recursively-in-python\n",
    "def reload_kglib() -> None:\n",
    "    with disabled_print():\n",
    "        deepreload.reload(kglib, exclude={key for (key, value) in sys.modules.items() if \"kagglelib\" not in key})\n",
    "\n",
    "np.set_printoptions(linewidth=200)\n",
    "pd.options.display.max_columns = None\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: where the truth lies OR # Guitar or Drums?\n",
    "# State of Machine Learning and Data Science 2020, Revisited\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "1. Methodology and key differences\n",
    "2. EDA for filtering \"pollution\", spam and user error\n",
    "3. Data Scientists' Profile\n",
    "4. Data Scientists' Programming and ML Experience\n",
    "5. Data Scientists' Salary\n",
    "6. Conclusion\n",
    "7. Appendix A: Data cleaning methodology, assumption, datasets\n",
    "8. Appendix B: Dedicated library and reusable code functions\n",
    "9. Appendix C: Exploring Q & A; suggestions for future Surveys\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Kaggle DS & ML Survey is an open online survey receiving thousands of responses from all over the world, offering unique insights. This analysis tries to achieve the following goals:\n",
    "\n",
    "- the identification of spam data, which is a fundamental principal of Exploratory Data Analysis (EDA)\n",
    "- examining the profile of the Data Scientists with an emphasis on the subject of compensation.\n",
    "\n",
    "The Kaggle DS & ML Survey being a global online survey is affected by factors that Machine Learning is often used to mitigate: *spam* and *user error*. According to the Survey Methodology, *spam* has already been excluded from the data. Nevertheless, using Exploratory Data Analysis (EDA) we find that a large part of the data should be removed from the dataset for being spam or for having abnormal, irregular values. Using relatively lenient criteria we manage to remove such observations without dropping possible outliers.  \n",
    "\n",
    "Our analysis comprises the following unique features:\n",
    "\n",
    "- Filtering the data, which is a principal feature and a goal in itself.\n",
    "- Grouping countries according to the World Bank \"Income Groups\" definition to account for the wide economic diversity.\n",
    "- Reconstructing the various classes (bins) in which the data are provided to account for their width and their underlying properties.\n",
    "- Excluding USA and India from global or group aggregates, thus offering a alternative view on the pronounced symmetric differences.\n",
    "\n",
    "Combining these four features, our results diverge to a large extend from the \"Executive Summary\", and all other analyses that do not filter the data, especially with respect to our focus, the issue of employees' compensation levels. Our findings add to the understanding of the data, providing information to students, professionals and interested companies in order to optimize their strategical behavior with respect to compensation, relocation and remote work. \n",
    "\n",
    "The key findings are:\n",
    "\n",
    "- There are at least 3861 observations (19.3% of the dataset) than need to filtered out.\n",
    "- Data Scientists tend to hold higher academic degrees compared to other occupations in the survey.\n",
    "- Median salaries are significantly higher and salary distribution is shifted towards higher ranges. For example, the median salary for India-based Data Scientists is double than what the unfiltered data suggest.\n",
    "- The age distribution density is much higher in younger ages and compared to the unadjusted bins.\n",
    "- US-based data scientists are more experienced and for the same levels of experience, they earn almost double compared to their colleages in other high income countries.\n",
    "- The Data Scientist salary increases significantly with Coding and Machine Learning Experience.\n",
    "\n",
    "The contributions of our work are:\n",
    "\n",
    "- A fundamentally different methodological framework on a number of issues.\n",
    "- The combination of several economic databases to create a new dataset that proxies the average salary in most countries of the world.\n",
    "- The creation of a dedicated python library that not only makes our analysis reproducible but also makes it easy to research different aspects of the dataset using the afore-mentioned methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we have created a dedicated python \"library\" with functions.  \n",
    "Our analysis code may be reused and reparameterized for different filtering thresholds and Survey subsets, for immediate reproduction of our analysis or for exploring moderate or stricter filtering scenarios.  \n",
    "The whole analysis (other than the text) can be adjusted by setting one different argument (or more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Last but not least, we have created a dedicated python \"library\" with functions.  \n",
    "Our analysis code may be reused and reparameterized for different filtering thresholds and Survey subsets, for immediate reproduction of our analysis or for exploring moderate or stricter filtering scenarios.  \n",
    "The whole analysis (other than the text) can be adjusted by setting one different argument (or more).\n",
    "\n",
    "In Appendix A we discuss in detail our methodology and the datasets we use.\n",
    "\n",
    "In Appendix B, we provide another distinct result of our exploratory analysis;\n",
    "a concise list of suggestions for future surveys.\n",
    "\n",
    "# 1. Methodology and key differences\n",
    "\n",
    "\n",
    "### a) Exploring the questions, the answer choices and the dataset for spam, errors and unacceptable values\n",
    "\n",
    "The first part of our EDA is dedicated to exploring abnormal/invalid observation values that have not be flagged as spam by the survey system, but should be definetely excluded from the data.  \n",
    "We use the official \"Executive Summary\" as a benchmark but our results also differ from all other EDAs that explore the data without filtering them first.\n",
    "\n",
    "The number of unacceptable values and the criteria that we had to set, highlight the principal importance and challenges of exploring the data for errors and cleaning them before embarking on any other kind of analysis.\n",
    "\n",
    "Besides unacceptable values, we find mispecifications in \"Questions\" and \"Answer Choices\" that may lead to unfounded conclusions if they go unnoticed.\n",
    "\n",
    "\n",
    "### b) Adjusting for cross-country economic differences\n",
    "\n",
    "The importance of meaningful information on compensation levels cannot be understated in a field that transcends national borders like few others.  \n",
    "Nevertheless, the differences among different economies are so profound that a global view on Data Scientist salary distribution shows an \"everything goes\" result.\n",
    "\n",
    "Besides a robust single country median calculation, we group the countries according to the World Bank Income Groups and explore the salary distribution per group of countries. This allows us in turn to explore in a meaningful way the effect that experience levels have on salary in each countries income group. We select \"experience\" as the variable that may explain differences in salary levels and find that it leads to significant within-group differences for all country groups and countries.\n",
    "\n",
    "\n",
    "### c) Reconstruction of data aggregated bins (classes)\n",
    "\n",
    "Using different classes width for aggregating observations is convenient and informative.  \n",
    "But convenience comes with a trade-off; a large amount of information is lost.  \n",
    "If bins are created unequal, this may lead to \"over\" or \"under\" representation of data, depending on the relative width difference and the number of observations.  \n",
    "It may  also distort provided information and create optical \"illusions\" in visual representations.\n",
    "\n",
    "We reconstruct bins using various ways where we find this phenomenon to exist in the data and we gain new and useful information.\n",
    "\n",
    "\n",
    "### d) Examine USA versus Rest of World (RoW) symmetric difference\n",
    "\n",
    "When comparing the aggregate metrics of a subset of the data with the overall aggregates we examine whether the subset in question influences heavily the aggregated values.  \n",
    "It is informative to compare a subset with low number of observations against the total, to asses how it measures against it.\n",
    "\n",
    "But, for a subset with substantial weight, information is distorted since the subset defines the total to a significant degree.  \n",
    "In that case, comparing the symmetric difference can add valuable information for both sides. Therefore, when comparing the USA, we exclude it from the total.\n",
    "\n",
    "## 2. EDA for filtering \"pollution\", spam and user error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unfiltered Kaggle data with optimised labels andcolumns for EDA\n",
    "udf = kglib.load_udf()\n",
    "\n",
    "seconds = [30, 45, 60, 90]\n",
    "super_fast_participants = pd.DataFrame(\n",
    "    {\n",
    "        \"seconds\": seconds,\n",
    "        \"fast_participants\": [len(udf[udf.duration <= sec]) for sec in seconds],\n",
    "    }, \n",
    ")\n",
    "# super_fast_participants\n",
    "\n",
    "dataset = udf\n",
    "fastest = dataset.duration.nsmallest(1).values[0]\n",
    "fastest_time_n = dataset.duration.nsmallest(300).tail(1).values[0]\n",
    "slowest = dataset.duration.nlargest(1).values[0]\n",
    "slowest_time_n = dataset.duration.nlargest(300).tail(1).values[0]\n",
    "#fastest, fastest_time_n, (slowest/3600),  (slowest_time_n/3600)\n",
    "\n",
    "# (udf.duration <= 3000).sum()\n",
    "# udf[udf.duration < 3000].duration.hist(bins=200, grid=False)\n",
    "# udf.duration.median()\n",
    "# udf.duration.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference of our analysis is that the first part of our EDA is dedicated to exploring abnormal/invalid observation values that have not be flagged as spam by the survey system, but should definetely be excluded from the data.  \n",
    "The number of unacceptable values and the criteria that we had to set, highlight the complexity, the challenges but mostly the principal importance of exploring the data and cleaning them before embarking on any other kind of analysis.  \n",
    "Examining the very first variable we find that there are 347 participants that completed the survey in less than 60 seconds.  That is arguably not enough time to read the questions.\n",
    "\n",
    "| threshold (sec) | No. participants |\n",
    "|-----------------|------------------|\n",
    "| 30              |  29              |\n",
    "| 45              | 166              |\n",
    "| 60              | 347              |\n",
    "| 90              | 630              |\n",
    "\n",
    "\n",
    "\n",
    "The fastest \"participation\" time is 20 seconds, the 300th faster is 56 seconds, meaning that 300 participants \"completed\" the Survey in 56 seconds or less.  \n",
    "The slowest survey completion time is 318 hours (almost two weeks) and the 300th slowest time is 48 hours.  \n",
    "300 participants \"completed\" the Survey in more than two days.  \n",
    "Nevertheless, instead of choosing a reasonable minimum and maximum duration thresholds, we use other qualitative criteria, and we manage to filter out a large part of these observations.\n",
    "\n",
    "### The criteria we use to clean the data are:\n",
    "\n",
    "### Criterion 1: Participants who did not actually participate in the survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf.columns[:7].values\n",
    "\n",
    "orig_df = kglib.load_orig_kaggle_df()\n",
    "temp_df = orig_df.iloc[:, 7:]\n",
    "\n",
    "only_answer_demographic = (\n",
    "    (temp_df == 'None')\n",
    "    | temp_df.isnull()\n",
    ").all(axis=1)\n",
    "\n",
    "#len(orig_df[only_answer_demographic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion 1: Participants who did not actually participate in the survey.\n",
    "This includes observations with values only for the first set of general demographic questions and no other relevant information that adds any value concerning DS & ML.  \n",
    "We drop these irrelevant observation values as they do not provide further information relative to the scope of the survey.  \n",
    "We could also add a threshold for minimum 3 Not-Nan values, but being lenient, we merely drop those who only answered demographic questions, plus the next one and then quit the survey.\n",
    "\n",
    "Using our first criterion, we identify and drop 1082 observations.  \n",
    "Note that, not using criterion 1 does not affect our main results with respect to salary levels at all.\n",
    "\n",
    "### Criterion 2: Participants that are too young for their experience (in Programming or Machine Learning).\n",
    "\n",
    "#### Below we show some examples of that criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_too_young_for_experience = (\n",
    "    (udf.age <= \"24\")\n",
    "    & (udf.code_exp == \"20+\")\n",
    ")\n",
    "udf[is_too_young_for_experience].iloc[:,1:7].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undoubtly, it is impossible to be 24 years old or less and have 20+ years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion 3: Participants whose salary, experience, age and country of residence are mutually exclusive.\n",
    "\n",
    "As an example, it is impossible to be 24 years old or less, working as an employee (for a salary) and earn above 500,000 usd yearly, in any country in the world.  \n",
    "A closer examination of these observations shows that spam often is extensive.  \n",
    "\n",
    "#### Below we show some examples of that criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_young_high_salary = udf[\n",
    "    (udf.age <= \"24\")\n",
    "    & (udf.salary.isin([\"300000-499999\", \"500000-999999\"]))\n",
    "].dropna(axis=1).head()\n",
    "too_young_high_salary.iloc[:,1:11].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, as shown in the data below,  \n",
    "it is impossible to reside in the US, have more than 10 years of experience and earn less than 1000 usd per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_exp_bins = [\"10-20\", \"20+\"]\n",
    "is_high_exp = (\n",
    "    udf.code_exp.isin(high_exp_bins)\n",
    "    | udf.ml_exp.isin(high_exp_bins)\n",
    ")\n",
    "# udf[is_high_exp]\n",
    "\n",
    "extemely_low_salary_high_exp = (\n",
    "    (is_high_exp) &\n",
    "    (udf.salary_threshold <= 1000)\n",
    ")\n",
    "# udf[is_low_salary_high_exp].Mdropna(axis=1)\n",
    "\n",
    "USA_is_low_salary_high_exp = (\n",
    "    udf[\n",
    "        (extemely_low_salary_high_exp)\n",
    "        & (udf.country == \"USA\")\n",
    "    ].dropna(axis=1).iloc[:, 3:15]\n",
    ").dropna(axis=0).head(3)\n",
    "USA_is_low_salary_high_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple filtering we could drop hundreds of observations on the spot.  \n",
    "Beyond simple filtering, we account for the wide global differences and construct various combinations of age, salary, country and experience, taking the following steps:\n",
    "\n",
    "* a) we combine a comprehensive set of data sources to obtain an approximation for the annual average salary for each country (\"country_avg_salary\").  \n",
    "To counter data inavailabilty and benchmark our selection, we combine 6 different datasets.\n",
    "\n",
    "We use official average salary data from Eurostat for the European Union countries and the United Kingdom (UK).  \n",
    "For all the remaining countries, we use the World Bank Gross National Income per capita value, which is the measure used by the World Bank to define country \"Income groups\".  \n",
    "We compare them with WB Gross National Product per capita (GNI pc), with OECD average salary data, International Labour Organizaton average salary data and with unofficial \"numbeo\" data.  \n",
    "Due to lack of available data in International Organizations for Taiwan, we made a sinlge exception and used numbeo data.\n",
    "\n",
    "While it is expected that \"in average\" the salaries of occupation in this survey will be higher than the country average and the GNI pc, we intenionally do not adjust the salary threshold upwards but downwards to a considerable degree.\n",
    "\n",
    "We point out that this substituting GNI pc for salary data may not be a close approximation in all cases, therefore we adjust our thresholds considerably lower.  \n",
    "Our goal is not to a estimate the average salary of countries in the Survey, but to set a threshold with respect to Data Scientists and other occupations in the Survey, in order to clean the data and describe them afterwards.  \n",
    "Additionally, data (official or not) should be examined with caution and should not be taken as undeniable facts.  \n",
    "Not to forget, an average is as accurate as, you know, an average.  \n",
    "While it is expected that \"in average\" the salaries of occupation in this survey will be higher than the country average or the GNI pc, we intenionally do not adjust the salary threshold upwards but downwards to a considerable degree.\n",
    "\n",
    "We did not filter observations for which we could not obtain an approximation for the average salary, e.g. county \"Other\". \n",
    "\n",
    "Readers are kindly encouraged to read Appendix A, for a detailed discussion of the datasets, the method we apply and other possible alternatives.  \n",
    "\n",
    "An important final note is that due to the volume of inaccurate values and spam, the unfiltered data as such should not be used to calculate the average salary because this introduces the significant bias we aim to counter.  \n",
    "\n",
    "The various datasets we examined, the dedicated function and the average salary that we use for setting filtering threshods are shown below.  \n",
    "Readers are kindly encouraged to inspect the average salary we select for any country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_comparison_df = kglib.load_mean_salary_comparison_df()\n",
    "survey_countries = list(udf.country.unique())\n",
    "survey_salary_comparison = salary_comparison_df[salary_comparison_df.country.isin(survey_countries)]\n",
    "# survey_salary_comparison\n",
    "\n",
    "survey_salary_comparison[survey_salary_comparison.country.isin([\"USA\", \"India\", \"UK\", \"Brazil\", \"France\", \"Germany\", \"Spain\"])]                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining an average salary for each country, to avoid excluding outliers we set:\n",
    "\n",
    "* b) the corresponding \"salary_threshold\" for each observation, as the upper bound of Kaggle data salary bins.\n",
    "\n",
    "* c) the country-adjusted \"low_salary_high_exp\" threshold, two thresholds below the country average, applicable only for experience values greater or equal to 10 years. \n",
    "   \n",
    "* d) the country adjusted \"too_low_salary\" threshold, which we caclulate by multiplyiing the country average by 0.4 and then setting the rejection rate to be two thresholds below.  \n",
    "By multiplying the country average by 0.4 and then setting the relection threshold two thresholds below, we add two extra buffers to account for not participants which may not be 100% full-time employeed, for possible overestimation of the country average salary on our side, and for possible high salary variance in some coutries.\n",
    "    \n",
    "* e) the \"high_salary_low_exp\" threshold, which is above 300000 for all countries.\n",
    "\n",
    "Readers are kindly encouraged to read Appendix A, for a detailed summary of the filering criteria and conditions and to review the submitted code. \n",
    "\n",
    "As mentioned above, by multiplying the average salary and setting even lower thresholds, we leave room for part-time employeed participants.  \n",
    "Since, the question explicitly asks for yearly salary, observations that contain the value of very limited part-time salary as the yearly value, inhibit any meanigful exploration of proper yearly salary data.  \n",
    "To the extent that such values are present in the dataset, they are the result of user or design error and should be excluded from the data, even more so when exploring salary distributions and factors that might explain it.  \n",
    "\n",
    "To provide a comprehensive view of the data, and benchmark the filtering method, we make use of the unfiltered data and compare the difference between the two data sets (unfiltered, filtered).\n",
    "\n",
    "### In the table below we show a sample of the thresholds we calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kglib.get_threshold(65800, 0)\n",
    "# kglib.get_threshold(65800 * 1/3, 2)\n",
    "\n",
    "tresholds_df = kglib.load_thresholds_df()\n",
    "# tresholds_df[tresholds_df.country == \"USA\"]\n",
    "# tresholds_df[tresholds_df.country.isin(survey_countries_list)]\n",
    "\n",
    "tresholds_df[tresholds_df.country.isin([\"USA\", \"India\", \"UK\", \"Brazil\", \"France\", \"Germany\", \"Spain\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For country category \"Other\", we set the minimum thresholds, since the country names that would allow us to obtain each county's average salary are missing.  \n",
    "Our classification rules are intentionally too lenient and could most probably be much stricter.  \n",
    "This can be done on the spot, by setting different parameters in the relevant dedicated filtering function.  \n",
    "We explored various scenaria and we demonstrate a scenario on the very lenient spectrum, with respect to filtering.  \n",
    "Nevertheless, even with minimum strictness we get significantly different results than any other EDA that uses the data as is.  \n",
    "\n",
    "\n",
    "The significance of the values we drop, depends on the overall size of the subset they belong to and the respective calculated metric.  \n",
    "For example, a salary with value equal to 500,000 outweighs 100 observations with salry value equal to 5000.  \n",
    "Similarly, dropping 10 observations from a range of 15 observations, means that the size of the category is one third of its initial unfiltered size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = kglib.filter_df(udf, print_filters=False)\n",
    "\n",
    "filtered_obs_size = len(udf) - len(fdf)\n",
    "# filtered_obs_size,  filtered_obs_size/len(udf), len(fdf), len(udf)\n",
    "\n",
    "dataset = fdf\n",
    "fastest = dataset.duration.nsmallest(1).values[0]\n",
    "fastest_time_n = dataset.duration.nsmallest(300).tail(1).values[0]\n",
    "slowest = dataset.duration.nlargest(1).values[0]\n",
    "slowest_time_n = dataset.duration.nlargest(300).tail(1).values[0]\n",
    "# fastest, fastest_time_n, (slowest/3600),  (slowest_time_n/3600)\n",
    "\n",
    "\n",
    "pros_fdf = fdf[\n",
    "    (fdf.role != \"Student\")\n",
    "    & ((fdf.role != \"Currently not employed\"))\n",
    "]\n",
    "\n",
    "dataset = pros_fdf\n",
    "fastest = dataset.duration.nsmallest(1).values[0]\n",
    "fastest_time_n = dataset.duration.nsmallest(300).tail(1).values[0]\n",
    "slowest = dataset.duration.nlargest(1).values[0]\n",
    "slowest_time_n = dataset.duration.nlargest(300).tail(1).values[0]\n",
    "# fastest, fastest_time_n, (slowest/3600),  (slowest_time_n/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the values that we filter are part of the same observation row, a fact that adds support to the decision to exclude them from the data.  \n",
    "As a result, the number of observations that we drop using each filter depends on the order the filters are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In total, we drop 3861 observations, 19.3% of the unfiltered data set.\n",
    "\n",
    "#### The filtered data contain 16175 rows.\n",
    "\n",
    "After filtering the data, duration times are not so extreme, but we still have values that could be dropped, even if we set lenient min and max time criteria.  \n",
    "The two weeks completion time observation is still in the data, the fastest participant took 48 seconds, 300 participants made it in 123 seconds or less.\n",
    "\n",
    "If we exclude \"Students\" and \"Curenty not Employeed\" duration times are more reasonable.  \n",
    "The fastest participant completed the Survey in 49 seconds, the 300 faster participants took less than 5 minutes and 7 seconds and the 300 slower made it after thinkiing about it for more than 12,2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Scientists Demographic Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uds = kglib.load_role_df(udf, role=\"Data Scientist\")\n",
    "fds = kglib.load_role_df(fdf, role=\"Data Scientist\")\n",
    "udf, uds, fdf, fds = map(kglib.keep_demo_cols, (udf, uds, fdf, fds))\n",
    "dataset1, dataset2 = uds, fds\n",
    "# len(fdf), len(fds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After filtering the data, the dataset contains 16175 observations.\n",
    "#### of which, 2080 are Data Scientists (DS).\n",
    "\n",
    "## a. Gender\n",
    "\n",
    "#### Men percentage is 1% higher compared to the unfiltered data,\n",
    "marginally better in terms of gender gap than the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1, dataset2 = uds, fds\n",
    "column = \"gender\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"No. participants\",\n",
    "    as_percentage=True,\n",
    "    order=[\"Man\", \"Woman\", \"Nonbinary\", \"No answer\", \"Self-describe\"],\n",
    ")\n",
    "# df\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df=df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    legend_location=\"center right\",\n",
    "    title=\"DS Gender identity, %\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Age\n",
    "\n",
    "#### Data scientists in the workplace, in the age of 18-21, are not that many after all.\n",
    "More than 60% of Data Scientists are below the age of 35 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"age\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"No participants\",\n",
    "    as_percentage=True\n",
    ")\n",
    "# df\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=13,\n",
    "    height=8,\n",
    "    orientation=\"v\",\n",
    "    title=\"DS Age distribution, %\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining Survey Age bins\n",
    "There is something more interesting concerning the whole survey distribution if we examine the bin widths.  \n",
    "The age bin of \"18-21\" spans over 4 years, the bin \"22-24\" over 3 years, and the \"25-29\" bin spans over 5 years.  \n",
    "This ad hoc bin creation is informative if you focus on any of these bins separately, but distorts the comparison between the three.  \n",
    "The relative size of the \"18-21\" bin is by definition augmented, while the \"22-24\" bin is by definition much smaller than the \"25-29\" bin.\n",
    "\n",
    "Using, for simplicity, the average number of participants per year in each bin and assuming equal distribution among each bin years, we get a different story.  \n",
    "The information the \"adjusted bins\" graph conveys is that it can be read as a potential growth and popularity trend of Kaggle.\n",
    "\n",
    "Adjusting the age bins, we find that in the \"20-24\" cohort there are 30% more participants than in the \"25-29\" cohort, and this is a noteable future growth trend prediction.  \n",
    "Plus, there are a lot more participants in the \"18-19\" age cohort than meets the eye.   \n",
    "Note that the \"18-19\" bin spans over two years.   \n",
    "Again, taking the average number of participants per year, the \"18-19\" bin density is definetely more than 10% greater than the \"25-29\" bin average, showing that Kaggle is way more popular to \"newcomers\".  \n",
    "The results shown below hold whether we use filtered or unfilterd data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fdf\n",
    "kglib.sns_plot_age_distribution(\n",
    "    dataset,\n",
    "    width=14,\n",
    "    height=9,\n",
    "    title=\"Age distribution of Survey participants, Default vs Adjusted bins, Filtered\"\n",
    ")\n",
    "\n",
    "#for unfiltered data we set:\n",
    "# dataset = udf\n",
    "# kglib.sns_plot_age_distribution(dataset, width=14, height=8,title=\"Age distribution of Survey participants, Default vs Adjusted bins, Uniltered data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. country\n",
    "\n",
    "####  The difference between USA and India is narrow, the UK is third in terms of participants and the Netherlands are included in the top 15 countries. \n",
    "\n",
    "Ps. There are two different \"answer choices\" for Korea, 76 obs for \"Republic of Korea\" and 190 for \"South Korea\".  \n",
    "Both are associated with South Korea and we aggregate them as \"Korea, Republic of\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Korea = orig_df[orig_df.Q3.str.contains(\"Korea\")].groupby('Q3').size()\n",
    "# Korea\n",
    "\n",
    "dataset1, dataset2 = uds, fds\n",
    "column = \"country\"\n",
    "df = kglib.get_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    as_percentage=True\n",
    ")\n",
    "\n",
    "df = df[df.Filtered > 1.48]\n",
    "df = df.sort_values(by=\"Filtered\")\n",
    "df = kglib.stack_value_count_comparison(df, \"participants (%)\")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=14,\n",
    "    height=8,\n",
    "    orientation=\"v\",\n",
    "    order_by_labels=False,\n",
    "    x_ticklabels_rotation=30,\n",
    "    title=\"DS, top 15 countries, %\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Education\n",
    "\n",
    "#### After filtering the data, \"possible\" PhD holders are increasd by more than 13.5%,\n",
    "reaching the 20% of the total population. That is quite impressive for that level of education.  \n",
    "Combined with the fact that the Masters also increase slightly and that Bachelors decrease by around 13% we find that Bachelors and PhD distribution is almost equal.  \n",
    "Keep in mind that this is indeed a distibution skewed towards top education levels, quite rare in other occupations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1, dataset2 = uds, fds\n",
    "column = \"education\"\n",
    "order = [\n",
    "    'No answer',\n",
    "    'High school'\n",
    "    'Studies without a degree',\n",
    "    'Professional',\n",
    "    'Doctoral',\n",
    "    'Bachelor’s',\n",
    "    'Master’s',\n",
    "]\n",
    "\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=order,\n",
    ")\n",
    "# df\n",
    "\n",
    "reload_kglib()\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"DS % of Education or Educational Goals?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick view on the relevant education distribution of Data Analysts (DA) will convince you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uda = kglib.load_role_df(udf, role=\"Data Analyst\")\n",
    "fda = kglib.load_role_df(fdf, role=\"Data Analyst\")\n",
    "uda, fda = map(kglib.keep_demo_cols, (uda, fda))\n",
    "\n",
    "dataset1, dataset2 = uda, fda\n",
    "column = \"education\"\n",
    "order = [\n",
    "    'No answer',\n",
    "    'High school'\n",
    "    'Studies without a degree',\n",
    "    'Professional',\n",
    "    'Doctoral',\n",
    "    'Bachelor’s',\n",
    "    'Master’s',\n",
    "]\n",
    "\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=order,\n",
    ")\n",
    "# df\n",
    "\n",
    "reload_kglib()\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"DA % of Education or Educational Goals?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the questions asked:\n",
    "There is another important discovery though.  \n",
    "The question is set in a way that does not allow for definite inferences about the current level of education of participants.  \n",
    "Even if \"Data Scientists\" are not \"Students\", we can not be 100% certain whether they replied about the level of education they have already attained or about the one they plan to attain.  \n",
    "#### Therefore, we exclude this very interesting dimension of education from our analysis on salaries as any EDA based on information provided by it would be unfounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Scientists Programming and ML Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Programming Experience USA vs RoW\n",
    "\n",
    "#### The percentage of Data Scientitsts in the US with more than 20 years of Programming Experience is double than in the RoW.  \n",
    "Similarly, more than the two thirds of Data Scientists in the US, have more than 5 years of experience.  \n",
    "On the other hand, there is a shortage of Experienced Data Scientists in the RoW, since more than 50% have less than 5 years of Programming Experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_usa = fds[fds.country == \"USA\"]\n",
    "fds_non_usa = fds[fds.country != \"USA\"]\n",
    "dataset1 = fds_usa\n",
    "dataset2 = fds_non_usa\n",
    "\n",
    "column = \"code_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    ")\n",
    "\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Programming XP, USA vs RoW %, Filtered\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Using the unfiltered data and excluding the US from the global aggregate we find that:  \n",
    "#### the percentage of Data Scientitsts in the US with more than 20 years of Programming Experience is almost triple than in the RoW.  \n",
    "The shortage of experienced data scientist in the RoW is manifested vividly, as more than 60% of Data Scientists in the RoW have less than 5 years of Programming Experience.  \n",
    "\n",
    "\n",
    "Ps. There is an error in the \"Executive Summary\", p.9 plot concerning the USA \"20+ years\" bin.  \n",
    "The plot shows 7.6% of participants while the actual unfiltered data value is 17.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uds_usa = uds[uds.country == \"USA\"]\n",
    "uds_non_usa = uds[uds.country != \"USA\"]\n",
    "dataset1 = uds_usa\n",
    "dataset2 = uds_non_usa\n",
    "\n",
    "column = \"code_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    ")\n",
    "\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Programming XP, USA vs RoW %, Unfiltered\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Machine learning experience USA vs RoW\n",
    "\n",
    "After filtering the data and excuding the USA from the global aggragate, we find that:  \n",
    "#### The relative differences in Data Scientists with experience of more than 10 years is higher compared to the global aggreagares that include the US.\n",
    "\n",
    "More than one in three respondents (34.6%) from the US have experience of 10 years or more, versus one in five (20.3%) in the RoW.  \n",
    "\n",
    "Ps. There are two errors in the \"Executive Summary\", p.10 plot.  \n",
    "The bin for \"10-15 years\" should be \"10-20 years\".\n",
    "The relative scale for the global bars at the bins of \"10-20 years\" and \"20 or more\" has been set to almost zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_usa = fds[fds.country == \"USA\"]\n",
    "fds_non_usa = fds[fds.country != \"USA\"]\n",
    "dataset1 = fds_usa\n",
    "dataset2 = fds_non_usa\n",
    "\n",
    "column = \"ml_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    "    #order=order.reverse()\n",
    ")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Machine Learning XP, USA vs RoW %, Filtered\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the unfiltered data but excluding the USA from the global aggregates, we see that: \n",
    "#### Less than one in 6 respondents have experience of more than 5 years in the RoW (16.3%).  \n",
    "There is a relative reduction of around one fourth in the unfiltered versus the filtered data, for the  high experience categories.\n",
    "\n",
    "Similarly, only 4.5% of Data Scientists in the ROW have more than 5 years experience in Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uds_usa = uds[uds.country == \"USA\"]\n",
    "uds_non_usa = uds[uds.country != \"USA\"]\n",
    "dataset1 = uds_usa\n",
    "dataset2 = uds_non_usa\n",
    "\n",
    "column = \"ml_exp\"\n",
    "df = kglib.get_stacked_value_count_comparison(\n",
    "    sr1=dataset1[column],\n",
    "    sr2=dataset2[column],\n",
    "    label1=\"USA\",\n",
    "    label2=\"RoW\",\n",
    "    stack_label=\"participants (%)\",\n",
    "    as_percentage=True,\n",
    "    order=natsort.natsorted(udf[column].unique(), reverse=True)\n",
    "    #order=order.reverse()\n",
    ")\n",
    "\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df,\n",
    "    width=8,\n",
    "    height=8,\n",
    "    orientation=\"h\",\n",
    "    order_by_labels=False,\n",
    "    title=\"Machine Learning XP, USA vs RoW %, Unfiltered\",\n",
    "    palette=[sns.desaturate(\"green\", 0.75), \"peru\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Global Salary distribution\n",
    "The unfiltered data global salary distribution exhibits an \"anything goes\" pattern with three slightly higher density modes which are partly related to unequal bin widhts.  \n",
    "Note that the \"100000-124999\" bin width is 2.5 times wider than the \"90000-99999\" bin width.  \n",
    "Similarly, if we construct equal size bins by joining the \"7500-9999\" and the \"5000-7499\" bins, we will get higher values than the \"10000-14999\" bin.  \n",
    "Even worse, what literaly sticks out is the \"0-999\" bin suggesting that almost one in five Data Scientists globaly earn less than on thousand $USD per year.  \n",
    "\n",
    "Thankfully, this is related to spam, user-error or probably part-time employement values.  \n",
    "The picture is not as bleak after filtering the data.  \n",
    "The filtered data global salary distribution values increase significantly, _from 25 to 30 percentage points_, and the density of the three main modes and of their neighboring bins also increases relative to their unfiltered values.  \n",
    "\n",
    "The three modes indicate hidden patterns in the data resulting in this distribution shape.  \n",
    "Therefore, we examine the salary distribution:\n",
    "\n",
    "    a) of the countries with the higher number of participants in the Survey (USA, India, UK, Brazil). \n",
    "    These countries happen to belong in different World Bank Income groups.\n",
    "    \n",
    "    b) groups of countries according to the World Bank \"Income Group\" categories.  \n",
    "    There were no countries in the \"Lower\" income group in the data.  \n",
    "    Some participants from countries belonging to this group, might be have been classified in the \"Other\" country category.  \n",
    "    Unfortunately, this information is not available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    uds,fds,\n",
    "    width=8, height=8,\n",
    "    title=\"% of Global Salary distribution of Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. US salary distribution\n",
    "\n",
    "Examining the filtered data, we get a clearer idea about what to expect working as a data scientist in the US.  \n",
    "We should expect, 97% of the times, a minimum staring salary above 70000.  \n",
    "We find that the US salary distribution is even more concentrated in the 90k to the 200k range.  \n",
    "Resembling one standard deviation analysis without calculating the mean, we find that more than 70% of salary observations are in the 90k to the 200k range.  \n",
    "The reason we choose the 90-99k bin instead of the 200-249k bin in order to calculate the cumulative peercentage of 70% of observations has to do with the relative bin widths as discussed previously.  \n",
    "\n",
    "The 100-125k bin retains its dominant position even if we combine the three bins below it that surpass the 100-125k bin width if we joined them.  \n",
    "We conclude that the mode of the distibution is the 100k-124999 bin, while the median would probably lie in the 125k-14999 bin.  \n",
    "The 150-199k bin density is double that the 200-249k bin which has equal witdh but its width is double than the previous two bins.  \n",
    "While it is a fact that most of the obseravations (23%) are in this range, the other fact is that axis does not have equal scale, creating a kind of visual \"illusion\".  \n",
    "This should be noted so that readers get a better understaning of the data.  \n",
    "Comparing the mode midpoint (112.5k) that we infer by examing the data and the adjusted bins, we find that the mode mid-point is higher by two thirds compared to the average US country salary. (68700 + (68700*0.66)).  \n",
    "That is an information that all interested parties should keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_uds = uds[uds.country == \"USA\"]\n",
    "usa_fds = fds[fds.country == \"USA\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    usa_uds, usa_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 24), x2_limit=(0, 24),\n",
    "    title=\"% of Salary distribution of US-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c. India Salary distribution\n",
    "Being one fifth of the data we think it is necesary to focus a bit on more on the Salary distibution of India.\n",
    "\n",
    "Examining the filtered data, we find that the Data Scientists salary distribution in India spreads out along a wider range of values.  \n",
    "As expected, after excluding data that suggested that one third of Data Scientists earn less than 25% of the country average, we can make more reliable inferences.  \n",
    "Note that there are no observations in the top salary bins despite setting lenient rejection thresholds.  \n",
    "This highlights the importance of filtering criteria that amy capture a small part of total data, since the might play an important role when examining subsets.\n",
    "\n",
    "Taking into account the unequal bin size we can see that 69.3% of the observations are in the 4k-4999 range.  \n",
    "This resonates with the fact the India is classified by the World Bank as a \"Lower Middle\" income country.  \n",
    "\n",
    "\n",
    "When we calculate the cumulative percentages of the distribution, we do not include the 40k-49999 bin. \n",
    "Instead, we include the 4k-4999 bin, despite the fact that both bins have similar density.  \n",
    "We do so because the 40-49999 bin is 10 times wider.  \n",
    "Simalarly, by examining bin widths, we find that the distribution density mode is at the 5k-9999 area (15.7%) including almost 10% more obsrevations than the 10k-14999 bin.  \n",
    "\n",
    "Interestingly, the mode bin midpoint (7500) is almost four times larger than the country average salary we use to set the rejection threshold.\n",
    "\n",
    "This evidence adds support to the general intuition that Data Scentists' salaries are considerably higher than the average country salaries.  \n",
    "It also indicates that our salary value rejection criteria were lenient indeed.  \n",
    "As a final note, by examining the filtered data graph, we find evidence that being intentionally lenient, we have allowed for a part of non-normal values to remain in the data in the 1000-1999 bin.  \n",
    "This also infulences the salary distribution and indicates that it might actually take higher percentage values.  \n",
    "Pointing out non-normal values should be noted since the identification of such values is a goal in itself in any EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_uds = uds[uds.country == \"India\"]\n",
    "india_fds = fds[fds.country == \"India\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    india_uds, india_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 32.5), x2_limit=(0, 32.5),\n",
    "    title=\"% of Salary distribution of India-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d. UK salary distribution\n",
    "\n",
    "The UK salary distribution is also spread out, but a closer look reveals that the 150k-199999 bin sticks out partly due to its relatively large width.  \n",
    "Still, 36% of values are above the 100k threshold and there is a two-modal pattern in the UK distribution.  \n",
    "Despite being classified by the World Bank as a \"High Income\" country, the UK is actually among the top countries in that category in terms of average salary.  \n",
    "Accounting for experience levels could perhaps explain salary differences more than age or education.\n",
    "\n",
    "Overall, the UK Data Scientists salary distribution area occupies values much lower compared to the USA.  \n",
    "This fact, indicates that we should exclude the USA from the group of \"High Income\" countries when examining the salary distribution of the group.  \n",
    "We took a similar approach when we examined the experience levels in the USA versus the RoW.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_uds = uds[uds.country == \"UK\"]\n",
    "uk_fds = fds[fds.country == \"UK\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    uk_uds, uk_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 20), x2_limit=(0, 20),\n",
    "    title=\"% Salary distribution of UK-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d. Brazil salary distribution\n",
    "\n",
    "Brazil is classified by the World Bank as an \"Upper middle\" income country.\n",
    "\n",
    "The salary distributtion is significantly different than the other three countries we examined.\n",
    "\n",
    "We see that the majority of observations are in the 10k-59999 range and observation are more evenly distributed in the 10k to 40k range.\n",
    "\n",
    "The graph indicates that some possible spam and unacceptable values remain in the \"2000-2999\" and \"3000-3999\" bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brazil_uds = uds[uds.country == \"Brazil\"]\n",
    "brazil_fds = fds[fds.country == \"Brazil\"]\n",
    "\n",
    "kglib.sns_plot_global_salary_distribution_comparison(\n",
    "    brazil_uds, brazil_fds,\n",
    "    width=8, height=8,\n",
    "    x1_limit=(0, 20), x2_limit=(0, 20),\n",
    "    title=\"% Salary distribution of Brazil-based Data Scientists, $\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5e. Comparing country Medians\n",
    "\n",
    "Comparing the median salaries of the filtered versus the unfiltered data we find a significant difference for most countries.  \n",
    "\n",
    "Taking the bin midpoints, we find that the median salary is higher by:\n",
    "\n",
    "    44% in Japan,\n",
    "    22% in France,\n",
    "    \n",
    "    and double in Russia, Brazil, India,\n",
    "   than the unfiltered data suggest.  \n",
    "\n",
    "This graph confirms the signifiacnt difference between the US and other countries, in terms of Data Scientists salary.  \n",
    "There results are similar for the whole data set and not only for Data Scientists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kglib.load_salary_medians_df(\n",
    "    dataset1=uds,\n",
    "    dataset2=fds,\n",
    "    label1=\"Unfiltered\",\n",
    "    label2=\"Filtered\",\n",
    "    countries = [\"USA\", \"Germany\", \"Japan\", \"France\", \"Russia\", \"Brazil\", \"India\"],\n",
    ")\n",
    "\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df=df, \n",
    "    width=8, \n",
    "    height=8, \n",
    "    orientation=\"h\", \n",
    "    order_by_labels=False,\n",
    "    annotation_mapping=kglib.REVERSE_SALARY_THRESHOLDS,\n",
    "    title=\"DS Median salary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5f. Salary distributions for country \"Income groups\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the global salary distribution is so diverse, we group the countries in the data according to the World Bank (WB) \"Income group\" criteria, in order to examine possible differences in the distribution.  \n",
    "Please read Appendix A for relevant info on WB \"Income Groups\".  \n",
    "As a reminder, the data bins' width in the x-axis is not equal, in fact bins size increases by orders of magnitude.  \n",
    "We ameliorate this by using logarithmic scale and by adjusting the graph smoothing factor.  \n",
    "\n",
    "According to the WB definition the Income groups comprises countries with wide income differences.  \n",
    "For example, the WB \"High Income\" group definition includes Romania, Portugal, Germany, Switzerland and the USA.\n",
    "\n",
    "Ploting the salary by country income groups we get an informative decomposition of the distribution, in contrast to the \"everything goes\" global view.  \n",
    "We find that the US is a class of its own in the data, taking quite higher values than the rest of the \"High Income\" countries.  \n",
    "This is another confirmation that the USA merits a distinct examination and at the same, when making inference about the RoW the USA should be excluded.  \n",
    "Therefore, the \"High Income\" group in this graph does not include the USA.  \n",
    "The graph confirms the USA distribution mode is the one that we infer by examing the bins' width.  \n",
    "\n",
    "The main area of the \"High Income\" countries distribution appears to be after the mode of the \"Upper Middle\" income group but before the USA distribution mode.\n",
    "\n",
    "The \"Upper Middle\" countries distribution mode is positioned at the salary level of the mimimun outliers of the \"High Income\" distribution.  \n",
    "The left tail is most likely unfiltered spam.\n",
    "\n",
    "Since India comprises one fifth of the Data Scientists, we examine it separately and exclude it from the \"Lower Middle\" countries group to assess whether that makes a difference.  \n",
    "We find that India is somewhere in the middle between \"Upper Middle\" and \"Lowew Middle\" countries.  \n",
    "\n",
    "The \"Lower Middle\" countries salary levels are ditributed mostly to lower salary bins. The left tail might be related to spam or to countries with very low salary levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_kglib()\n",
    "\n",
    "dataset = fds\n",
    "\n",
    "dataset = dataset[~dataset.salary.isna() & ~(dataset.country == \"Other\")]\n",
    "usa = dataset[(dataset.country == \"USA\")].salary_threshold.reset_index(drop=True).rename(\"USA\")\n",
    "high = dataset[dataset.income_group.str.startswith(\"3\") & (dataset.country != \"USA\")].salary_threshold.reset_index(drop=True).rename(\"High\")\n",
    "upper_middle = dataset[dataset.income_group.str.startswith(\"2\")].salary_threshold.reset_index(drop=True).rename(\"Upper Middle\")\n",
    "india = dataset[(dataset.country == \"India\")].salary_threshold.reset_index(drop=True).rename(\"India\")\n",
    "lower_middle = dataset[dataset.income_group.str.startswith(\"1\") & (dataset.country != \"India\")].salary_threshold.reset_index(drop=True).rename(\"Lower Middle\")\n",
    "\n",
    "series = (usa, high, upper_middle, india, lower_middle)\n",
    "\n",
    "kglib.sns_plot_pde_comparison(series,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5g. Salary distributions for Income groups and occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fdf\n",
    "roles = [\n",
    "    \"Business Analyst\",\n",
    "    \"DBA/Database Engineer\",\n",
    "    \"Data Analyst\",\n",
    "    \"Data Engineer\",\n",
    "    \"Data Scientist\",\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"Research Scientist\",\n",
    "    \"Software Engineer\",\n",
    "    \"Product/Project Manager\",\n",
    "    \"Statistician\",\n",
    "    \"Other\",\n",
    "]\n",
    "series = list(map(lambda role: dataset[dataset.role == role].salary_threshold.dropna().reset_index(drop=True).rename(role), roles))\n",
    "kglib.sns_plot_pde_comparison(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP from here , IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kglib.load_median_salary_per_income_group_per_XP_level_df(uds, xp_type=\"code\", income_group=\"3\")\n",
    "kglib.load_median_salary_per_income_group_per_XP_level_df(uds, xp_type=\"ml\", income_group=\"3\")\n",
    "kglib.load_median_salary_per_income_group_per_XP_level_df(uds, xp_type=\"ml\")\n",
    "\n",
    "df = kglib.load_median_salary_comparison_df(uds, fds, xp_type=\"code\", income_group=\"3\")\n",
    "kglib.sns_plot_value_count_comparison(df, height=8, width=18, bar_width=0.35, title_wrap_length=80, title=\"Data Scientists: Median salary per Code XP level in High Income countries Filtered vs Unfiltered datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_usa = fds[fds.country == \"USA\"]\n",
    "fds_non_usa = fds[fds.country != \"USA\"]\n",
    "\n",
    "reload_kglib()\n",
    "df = kglib.load_median_salary_comparison_df(fds_non_usa, fds_usa, xp_type=\"ml\", income_group=\"3\", label1=\"Non USA DS\", label2=\"USA DS\")\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df, height=8, width=18, bar_width=0.35, title_wrap_length=70, \n",
    "    title=\"Median salary per Machile Learning XP level (filtered dataset)\"\n",
    ")\n",
    "\n",
    "df = kglib.load_median_salary_comparison_df(fds_non_usa, fds_usa, xp_type=\"code\", income_group=\"3\", label1=\"Non USA DS\", label2=\"USA DS\")\n",
    "kglib.sns_plot_value_count_comparison(\n",
    "    df, height=8, width=18, bar_width=0.35, title_wrap_length=70, \n",
    "    title=\"Median salary per Coding XP level (filtered dataset)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Data cleaning methodology, assumption, datasets, functions\n",
    "\n",
    "\n",
    "### a) Filtering tolerance\n",
    "\n",
    "\n",
    "The first thing that sticks out besides the Survey \"completion\" time is the large number of observations with salary value in the 0-999 bin.  \n",
    "\n",
    "As a general rule for this analysis, we intentionally set lenient criteria for rejecting \"spam\" or \"unacceptable values for two reasons:\n",
    "\n",
    "    a) In order to not lose information about an arguably high variance dataset, we aim to avoid dropping outliers that could be falsely indetified as spam, i.e. we want minimum, or even better, zero false \"spam\" (False Positives). \n",
    "    This implies that we allow observations that should be dropped to remain in the data, i.e we allow for false Negatives (classified as not spam, despite being spam).  \n",
    "    This rule that intentionaally sacrifices accuracy for retaining valuable information, is mainly selected for reason b).  \n",
    "    \n",
    "    b) Our method, analysis and results are unique and diverge significantly with the official \"Executive Summary\" and all other EDAs that do not filter the data.  \n",
    "    Therefore, setting very lenient spam identification criteria we are able to demonstrate without doubt that we don't \"cook the data\" to make a higher impact.\n",
    "    Spam and irregular values are so extensive in the data that one is tempted to drop more than one fourth of the dataset.  \n",
    "    Nevertheless, we demonstrate the validity of our analysis, using very lenient criteria, despite plenty of evidence that we could be more strict.  \n",
    "    For those who would like to see the results of using moderate or stricter criteria, we have created configurable dedicated functions that can be used to run all possible scenaria.\n",
    "\n",
    "\n",
    "### b) Straighfoward rejection conditions and criteria.\n",
    "\n",
    "\n",
    "Starting from \"question zero\", we examine the time that it took participants to complete the survey.\n",
    "\n",
    "Checking duration to complete and number of questions answered, we identified hundreds of observations that completed the survey in less than 30 seconds.  \n",
    "It is impossible to complete a survey of this length in 30 seconds, not even to read the questions asked.  \n",
    "\n",
    "Conclusion 1: the spam system method includes participants which did not actually complete the Survey or provided random  answers as fast as possible. Instead of choosing a reasonable minimum and maximum duration thresholds, we select other qualitative criteria, and we manage to filter out a large part of these observations.\n",
    "\n",
    "#### Criterion b1. Participants who did not actually participate in the survey.\n",
    "\n",
    "\n",
    "This includes observations with values only for the first set of general demographic questions and no other relevant information that adds any value concerning DS & ML.  \n",
    "We drop these irrelevant observation values as they do not provide further information relative to the scope of the survey.  \n",
    "We could also add a threshold for minimum 3 Not-Nan values, but being lenient, we merely drop those who only answered demographic questions, plus the next one and then quit the survey.\n",
    "\n",
    "Using our first criterion, we identify and drop 1082 observations.  \n",
    "Note that, not using criterion 1 does not affect our main results with respect to salary levels at all.\n",
    "\n",
    "\n",
    "### Criterion b2. Participants that are too young for their experience (in Programming or Machine Learning)\n",
    "\n",
    "it is impossible to be 24 years old or less and have 20+ years of experience.it is impossible to be 24 years old or less and have 20+ years of experience.\n",
    "\n",
    "\n",
    "\n",
    "Age VS Salary\n",
    "\n",
    "\n",
    "### Criterion 3: Participants whose salary, experience, age and country of residence are mutually exclusive.\n",
    "\n",
    "As an example, it argue that is impossible to be 24 years old or less, working as an employee (for a salary) and earn above 500,000 usd yearly, in any country in the world.\n",
    "### c) Salary data, assumptions and general scope.\n",
    "  \n",
    "To set a reliable salary cut-off threshold in order to evaluate observations values we use a step by step approach in which we set buffers to avoid dropping possible outliers. \n",
    "\n",
    "Undoubtly, an average salary comprises a range of values that fluctuate for a number of reasons, related to age, experience, cross-sectoral and within-sectoral differences, geographical differences within the same country and generally related to a whole range of factors that are beyond our scope to examine.  \n",
    "Additionally, the country average salary, GNI per capita and GDP per capita may me skewed towards higher values, for states with small total population and developed high-salary sectors.  \n",
    "On the other hand, for countries with very large population, in the lower incomer category and high percentages of unskilled labor, the national aggregates may be biased heavily towards smaller values.  This effect add spam tolerance to aour method and we alow for it by design.\n",
    "Our goal is not to a estimate the average salary of countries in the Survey, but to set a lenient threshold with respect to Data Scientists and other occupations in the Survey, in order to clean the data and describe them afterwards.\n",
    "\n",
    "Therefore, we combine a comprehensive set of data sources to obtain an approximation for the annual average salary for each country (\"country_avg_salary\").  \n",
    "To counter data inavailabilty and benchmark our selection, we combine 6 different datasets.\n",
    "\n",
    "    * For countries in the European Union, we [use Eurostat data.](https://ec.europa.eu/eurostat/databrowser/view/EARN_NT_NET__custom_414315/default/table?lang=en)  \n",
    "Specifically, we use \"Gross Annual Earnings of single person, without children earning 100% of the average earning, 2019, euro\".  \n",
    "We upload the whole eurostat dataset which comprises of fractions of average salary earnings. We use \"Sheet 8\".  \n",
    "The other candidate series are the \"Total\" and \"Net\" earnings.  \n",
    "The more relevant choice is \"Gross Earnings\".\n",
    "For a detailed account on Eurostat definitions please visit the official Eurostat [site.](https://ec.europa.eu/eurostat/cache/metadata/en/earn_net_esms.htm)\n",
    "\n",
    "    We have filtered other information columns and changed country names to align them across all other datasets for coding convenience.  \n",
    "    Since the question is about current yearly salary in US dollars, we convert euro values to USD, using the average 2019 exchange USD to EUR rate from the European Central Bank (ECB).  \n",
    "    We provide the relevant ECB file as well.\n",
    "\n",
    "    * For all the remaining countries, we use the \"World Bank Gross National Income per capita, current USD\" series, which is the measure used by the World Bank to define country \"Income groups\".  \n",
    "     \n",
    "We compare the selected average salary with the WB Gross National Product per capita (GNI pc), with OECD average salary data, which are reported in Purchasin Power Parity units, International Labour Organizaton average salary data and with unofficial \"numbeo\" data.  \n",
    "Due to lack of available data in International Organizations for Taiwan, we made a sinlge exception and used numbeo data.\n",
    "\n",
    "While it is expected that \"in average\" the salaries of occupation in this survey will be higher than the country average and the GNI pc, we intenionally do not adjust the salary threshold upwards but downwards to a considerable degree.\n",
    "\n",
    "We point out that this substituting GNI pc for salary data may not be a close approximation in all cases, therefore we adjust our thresholds considerably lower.  \n",
    "  \n",
    "Additionally, data (official or not) should be examined with caution and should not be taken as undeniable facts.  \n",
    "Not to forget, an average is as accurate as, you know, an average.  \n",
    "While it is expected that \"in average\" the salaries of occupation in this survey will be higher than the country average or the GNI pc, we intenionally do not adjust the salary threshold upwards but downwards to a considerable degree.\n",
    "\n",
    "We did not filter observations for which we could not obtain an approximation for the average salary, e.g. county \"Other\". \n",
    "\n",
    "Readers are kindly encouraged to read Appendix A, for a detailed discussion of the datasets, the method we apply and other possible alternatives.  \n",
    "\n",
    "An important final note is that due to the volume of inaccurate values and spam, the unfiltered data as such should not be used to calculate the average salary because this introduces the significant bias we aim to counter.\n",
    "\n",
    "Concering OECD data in PPP units, besides the USA which acts as a benchmark, for other countries this result in over or undee estimation of the average salary depending on the country price level.  \n",
    "For countries with higher price levels than the USA, the PPP OECD reported values are lower than the actual current nominal salary.  \n",
    "Conversely, for countries with lower price than the USA price levels, the PPP OECD reported values are higher than the actual current nominal salary.  \n",
    "The only OECD country not in the WB \"High Income\" group is Mexico.  \n",
    "For Iran, WB data still report the 2018 value.  \n",
    "Taiwan is a country for which there are no data available in the International Organizations that we examined.  \n",
    "We chose numbeo data for Taiwan and we would like to hear your view on the selected average salary with respect to Data Scientists or other survey related occupations.\n",
    "\n",
    "\n",
    "\n",
    "### d) Steps by step summary of rejection thresholds setting.\n",
    "\n",
    "#### Extensive examples of filtering by salary, age, experience and country:\n",
    "\n",
    "After selecting an average salary approximation, to avoid excluding outliers we set:\n",
    "\n",
    "* i) the corresponding \"salary_threshold\" for each observation, as the upper bound of Kaggle data salary bins.\n",
    "    \n",
    "    E.g. for a salary value in the \"4000-4999\" bin the salary threshold we set is \"5000\".\n",
    "\n",
    "* ii) the country adjusted \"low_salary_high_exp\" threshold, two thresholds below the country average, applicable only for experience value >= 10 years.\n",
    "    \n",
    "    E.g. for USA, average country salary is 65835, the bin threshold is \"60000-70000\" and the bin upper bound is 70000.  \n",
    "    So for participants from USA, with 10 or more years of experience, the \"low_salary_high_experience\" treshold is 50000.  \n",
    "    Accordingly, the rejection criterion drops values which are below the upper bin bound of 50000 (in the \"40000-49999\" range).  \n",
    "    In simple terms, for high experience, we drop observations with salary which are two thresholds below the country average salary upper threshold.\n",
    "   \n",
    "* iii) the country adjusted \"too_low_salary\" threshold. We multiply the country average by 0.33 and set the rejection rate to be two thresholds below.  \n",
    "    \n",
    "    E.g. for USA residents the average country salary is 65800 and 1/3 of the country average salary is 21945, which is in the \"20000-29999\" bin.  \n",
    "    Accordingly, the upper theshold is 30000 and the rejection criterion is applied for values two thresholds below, in the range of \"10000-14999\".  \n",
    "    In short, the \"too low salary\" rejection criterion drops values two thresholds below the threshold of the one third of the country average.\n",
    "\n",
    "* iv) the \"high_salary_low_exp\" threshold, which is above 300000 for all countries.\n",
    "\n",
    "    We argue that no one with zero or minimum experience is paid for working as a Data Scientist such a salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B. Setting arguments in the filtering function and related data exploraton functions.\n",
    "\n",
    "As mentioned above, the data are filtered using a dedicated function that can be reparameterized on the spot by setting:\n",
    "\n",
    "- a stricter (higher) or more lenient (lower) value for the \"low_salary_percentage\" argument,\n",
    "- a different value in the \"threshold_offset\" argument,\n",
    "- by adjusting what is considered to be high or low experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Data Analysis driven suggestions for future Kaggle Surveys.\n",
    "\n",
    "    1. Set a mimimum time rejection limit and a maximum \"timeout\" limit.\n",
    "    2. Request Age input as integer values.\n",
    "    3. Education titles. Ask a separate question about current obtained title.\n",
    "    4. Education current studies. Ask separately about current studies to \"Students\" only.\n",
    "    5. Education, future plans. If that is important for the scope of the Survey ask it separately.\n",
    "    4. If employeed first ask about full or part-time employment.\n",
    "    5. Request Monthly instead of Yearly salary and provide hints to participants.\n",
    "    6. Request Salary input as integer values\n",
    "    7. This is not always True: Non-professionals were defined as students, unemployed, and respondents that have never spent any money in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
